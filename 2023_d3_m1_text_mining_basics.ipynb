{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qoOltG3bKYpd"
   },
   "source": [
    "Preliminaries\n",
    "==============\n",
    "\n",
    "Mount your GDrive using the file browser OR run the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24599,
     "status": "ok",
     "timestamp": 1657709206938,
     "user": {
      "displayName": "Carl Stahmer",
      "userId": "08270031735613254632"
     },
     "user_tz": 420
    },
    "id": "22N_CuM-KpNM",
    "outputId": "97c33c56-707c-4764-b3fa-fff00dfebba4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTVkK-rFJrbw"
   },
   "source": [
    "From Text to Data\n",
    "==============\n",
    "\n",
    "We'll quickly review how to load, or \"read in,\" a single text file and format it for text analysis. We'll do so both as a \n",
    "refresher and because this simple action illuminates an important aspect of working with textual data: namely, that \n",
    "to your computer, text is above all a **sequence of characters**. This is a key thing to keep in mind when \n",
    "preparing your data for text mining and/or NLP.\n",
    "\n",
    "As you work through this chapter, use it as a check on your Python skills. If you feel comfortable writing the code \n",
    "below, you should be prepared for our sessions. The skills covered in this chapter include:\n",
    "\n",
    "+ Loading text data into Python\n",
    "+ Working with different Python data structures (strings, lists, dictionaries)\n",
    "+ Control flow with `for` loops\n",
    "+ Using `Pandas` dataframes\n",
    "\n",
    "```{tip}\n",
    "Need to brush up on Python? The DataLab offers a Python Basics workshop series. You can find links to the series \n",
    "reader and recording on our [Workshop Archive page].\n",
    "\n",
    "[Workshop Archive page]: https://datalab.ucdavis.edu/workshops/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jtmRiFGXJrb0"
   },
   "source": [
    "Loading Text\n",
    "---------------\n",
    "\n",
    "To open a text file, we'll use `with...open`. This saves us from forgetting to close the file stream and thereby \n",
    "frees up a little memory for later computation. The memory strain a single text file puts on your computer isn't \n",
    "very large at all, but dozens, to say nothing of hundreds or thousands of texts, can start to slow things down, so \n",
    "it's good to get in the habit of automatically closing file streams right from the start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 939,
     "status": "ok",
     "timestamp": 1657709224895,
     "user": {
      "displayName": "Carl Stahmer",
      "userId": "08270031735613254632"
     },
     "user_tz": 420
    },
    "id": "Ccd_YiltJrb1"
   },
   "outputs": [],
   "source": [
    "with open(\"/content/drive/MyDrive/rbs_digital_approaches_2022/2022_data_class/tm_1/shelley_frankenstein.txt\", 'r') as f:\n",
    "    frankenstein = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FICDNu8Jrb2"
   },
   "source": [
    "### Plain text\n",
    "\n",
    "Here we use `r` in the `mode` argument because we're working with **plain text** data (as opposed to **binary** \n",
    "data, which would require `rb`). In computing, plain text has multiple fuzzy, interlocking meanings, but generally \n",
    "it refers to some kind of data that is stored in a human-readable form, which is to say, it is comprised of a \n",
    "collection of text characters (usually [ASCII], but increasingly [UTF-8]).\n",
    "\n",
    "[ASCII]: https://en.wikipedia.org/wiki/ASCII\n",
    "[UTF-8]: https://en.wikipedia.org/wiki/UTF-8\n",
    "\n",
    "All the texts we'll be working with are plain text files. But depending on your research area, access to plain text \n",
    "representations of documents may be the exception, not the rule. If that's the case, you would need to convert \n",
    "your documents into a machine-readable form. Options for doing so range from automated methods, like [optical \n",
    "character recognition], to good old fashioned hand transcription.\n",
    "\n",
    "[optical character recognition]: https://en.wikipedia.org/wiki/Optical_character_recognition\n",
    "\n",
    "In plain text representations, every keystroke you would use to hand-transcribe a text has a corresponding sequence \n",
    "of characters. That means this print output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A5Sv9REoJrb2"
   },
   "outputs": [],
   "source": [
    "print(frankenstein[:364])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6ZXUKlDJrb3"
   },
   "source": [
    "...is represented by the following in plain text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "executionInfo": {
     "elapsed": 126,
     "status": "ok",
     "timestamp": 1657709619077,
     "user": {
      "displayName": "Carl Stahmer",
      "userId": "08270031735613254632"
     },
     "user_tz": 420
    },
    "id": "Ik7QaIVfJrb3",
    "outputId": "c3094fdf-ca8b-4489-a129-1f12a0bb26fc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Letter 1\\n\\n_To Mrs. Saville, England._\\n\\n\\nSt. Petersburgh, Dec. 11th, 17â€”.\\n\\n\\nYou will rejoice to hear that no disaster has accompanied the\\ncommencement of an enterprise which you have regarded with such evil\\nforebodings. I arrived here yesterday, and my first task is to assure\\nmy dear sister of my welfare and increasing confidence in the success\\nof my undertaking.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frankenstein[:364]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2iKP8riJrb4"
   },
   "source": [
    "```{margin} You might ask...\n",
    "Why aren't spaces also represented by a character? Well, they are, but even in this relatively unformatted view \n",
    "your computer continues to automatically render text in a human-readable way.\n",
    "```\n",
    "\n",
    "See all the `\\n`, or **newline**, characters? Each one represents a linebreak. On the backend, your computer uses \n",
    "newline characters to demarcate things like paragraphs, stanzas, titles, and so forth, but typically it suppreses \n",
    "these characters when it renders text for our eyes. What we see in the two different outputs above, then, is a \n",
    "difference between **print conventions** and **code conventions**: what appears as a blank in the former is in fact \n",
    "an addressable unit in the latter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EaBELM-zJrb5"
   },
   "source": [
    "### Character sequences\n",
    "\n",
    "The distinction between print and code conventions has some significant consequences for us, both practical and \n",
    "conceptual. While, in the print view of the world, we tend to think of the word as the atomic unit of text, in the \n",
    "code view of the world, text is -- again -- a **sequence of characters**. We can see this if we try to count how \n",
    "many units are in our text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 130,
     "status": "ok",
     "timestamp": 1657709647536,
     "user": {
      "displayName": "Carl Stahmer",
      "userId": "08270031735613254632"
     },
     "user_tz": 420
    },
    "id": "5akTYiJUJrb6",
    "outputId": "8949cffb-9721-44ec-e826-ae26d6ba01ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of Frankenstein is: 418917\n"
     ]
    }
   ],
   "source": [
    "print(\"The length of Frankenstein is:\", len(frankenstein))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xapJELQrJrb6"
   },
   "source": [
    "The Penguin edition of _Frankenstein_ clocks in at ~220 pages. If we assume each page contains something like 350 \n",
    "words, that makes the book ~77,000 words long -- far less than the number outputted above. Why, then, did Python \n",
    "output this number? Because it counted characters, not words. To Python, our text is currently represented as a \n",
    "giant blob. This blob makes no distinction between the start of one word and the next; its atomic unit is the \n",
    "character, and so most of the operations we can run on it will thus address characters, not words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_mwP2aQJrb6"
   },
   "source": [
    "### Tokenizing strings\n",
    "\n",
    "But we want to address our text at the level of words. To do so, we'll need to manipulate how Python represents our \n",
    "data, changing it from a long stream of characters to discrete (and preferably indexed) units. The process of doing \n",
    "this is called **tokenization**. _To tokenize_ means to break a continuous sequence of text data into substrings, \n",
    "which we call \"tokens.\" Ultimately, tokens are what we will end up counting in text analytics. They are the atomic \n",
    "unit of/for almost everything we'll discuss in this series.\n",
    "\n",
    "Notably, a token is more of a generic entity than it is a particular kind of text. Tokens don't always mean words \n",
    "(though you'll often see them treated this way). In one sense, for example, our text is already tokenized -- it's \n",
    "just tokenized by characters, which isn't much use for us now. What we want to do, then, is tokenize our text in \n",
    "such a way that we can address each word therein. This will help us keep track of those words, rather than mucking \n",
    "around with blobby character data.\n",
    "\n",
    "There are a number of different Python libraries that can tokenize text for you, but it's easy enough to do one \n",
    "version of this task with Python's base functionality. For now, we'll simply use `split()`. The default character \n",
    "this function takes in its argument is any whitespace, which will nicely isolate words (whitespace characters \n",
    "include `\\n`, `\\t`, and of course plain old spaces). We'll call `split()` on our text and save the result to `doc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 126,
     "status": "ok",
     "timestamp": 1657709661630,
     "user": {
      "displayName": "Carl Stahmer",
      "userId": "08270031735613254632"
     },
     "user_tz": 420
    },
    "id": "upZSjkWIJrb7"
   },
   "outputs": [],
   "source": [
    "doc = frankenstein.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6q-Ygv76Jrb7"
   },
   "source": [
    "Now, if we call `len()` on `doc`, we'll see this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 248,
     "status": "ok",
     "timestamp": 1657709686006,
     "user": {
      "displayName": "Carl Stahmer",
      "userId": "08270031735613254632"
     },
     "user_tz": 420
    },
    "id": "hWotUSxdJrb7",
    "outputId": "c03d44b8-f2de-4941-fc9e-8f452a528abb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of Frankenstein is: 74975\n"
     ]
    }
   ],
   "source": [
    "print(\"The length of Frankenstein is:\", len(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eeleBjmQJrb8"
   },
   "source": [
    "Much better! Now that we have tokenized by words, this number is considerably closer to our estimations above.\n",
    "\n",
    "```{admonition} A look ahead\n",
    "While you'll most often tokenize on whitespaces, there are cases where you might want to chunk your text using \n",
    "different characters, or even entire sequences of characters. For example, if you are studying poetry, you might \n",
    "want to know some information about the average number of lines in a stanza. In that case, splitting on `\\n` could \n",
    "be more useful than space. We'll cover this topic more fully in a later section; for the moment just keep in mind \n",
    "that there are many different and valid ways to tokenize text.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6LioqteRJrb8"
   },
   "source": [
    "Counting Words\n",
    "-------------------\n",
    "\n",
    "With our text data loaded and properly formatted, we can start one of the core tasks of text analysis: counting words. While the next chapter will discuss this process in greater detail, we'll preview it here to get a sense of \n",
    "what's to come and to review the basics of control flow in Python.\n",
    "\n",
    "Splitting text transforms it into a list, where each word has its own separate index position. Remember that, in \n",
    "Python, lists are ordered arrays that store multiple, potentially repeatable values. With this representation of \n",
    "our data, it's much easier get global word counts using something as simple as a `for` loop: we can simply iterate \n",
    "through every item in the list and tally them all up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrF8KWHYJrb8"
   },
   "source": [
    "### A first pass\n",
    "\n",
    "Let's do this now. We can build a little loop to find the cumulative number of times each word occurs in \n",
    "_Frankenstein_. To store this information, we'll use a dictionary. This will provide us with a way to access the \n",
    "counts of individual words once we've looped through the entire novel.\n",
    "\n",
    "```{margin} What this loop does:\n",
    "For every word in the novel, check whether that word is in the dictionary:\n",
    "+ If it isn't in the dictionary, add that word and count `1`\n",
    "+ If it is, increase that word's count by `1`\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 141,
     "status": "ok",
     "timestamp": 1657709765549,
     "user": {
      "displayName": "Carl Stahmer",
      "userId": "08270031735613254632"
     },
     "user_tz": 420
    },
    "id": "03wBk4EcJrb8"
   },
   "outputs": [],
   "source": [
    "word_counts = {}\n",
    "\n",
    "for word in doc:\n",
    "    if word not in word_counts:\n",
    "        word_counts[word] = 1\n",
    "    else:\n",
    "        word_counts[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZoW6EnZrJrb8"
   },
   "source": [
    "With this done, we can determine the total number of unique words in _Frankstein_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 269,
     "status": "ok",
     "timestamp": 1657709810901,
     "user": {
      "displayName": "Carl Stahmer",
      "userId": "08270031735613254632"
     },
     "user_tz": 420
    },
    "id": "X3IKIZJ3Jrb9",
    "outputId": "b7d97d54-5154-4bdd-fc68-6d342fc5deff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words in Frankenstein: 11590\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of unique words in Frankenstein:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVIKy7RqJrb9"
   },
   "source": [
    "And we can also access the counts of individual words. Let's pick two: \"imagination\" and \"monster.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 151,
     "status": "ok",
     "timestamp": 1657709830377,
     "user": {
      "displayName": "Carl Stahmer",
      "userId": "08270031735613254632"
     },
     "user_tz": 420
    },
    "id": "3YbPr3vAJrb9",
    "outputId": "0e722427-bdb9-43c3-b7ab-9d479e990c80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imagination  14\n",
      "monster      21\n"
     ]
    }
   ],
   "source": [
    "for word in [\"imagination\", \"monster\"]:\n",
    "    print(f\"{word:<12} {word_counts[word]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GSXaVBUFJrb9"
   },
   "source": [
    "```{tip}\n",
    "For the sake of readability, this reader uses extra string formatting to control the print spacing of our output. \n",
    "This isn't necessary though, so feel free to work without it. If you do want to use the extra formatting, you can \n",
    "do so by appending any print string with `f`. Then, use `{}` around variables that you'd like to interpolate into \n",
    "the string. `:<[NUMBER]` and `:>[NUMBER]` will control left and right justification, respectively.\n",
    "```\n",
    "\n",
    "If you're familiar with _Frankenstein_, you'll know that it's an epistolary novel, meaning it's written as a series \n",
    "of letters. It even begins this way: the heading in the first print output above reads \"Letter 1.\"\n",
    "\n",
    "With that in mind, let's tack on \"letter\" to our loop above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 161,
     "status": "ok",
     "timestamp": 1657709870808,
     "user": {
      "displayName": "Carl Stahmer",
      "userId": "08270031735613254632"
     },
     "user_tz": 420
    },
    "id": "QpmHJbQTJrb9",
    "outputId": "7a09ccb8-acba-4dea-99ab-f950fdf80f4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imagination  14\n",
      "monster      21\n",
      "letter       17\n"
     ]
    }
   ],
   "source": [
    "for word in [\"imagination\", \"monster\", \"letter\"]:\n",
    "    print(f\"{word:<12} {word_counts[word]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYFqKZgTJrb-"
   },
   "source": [
    "### Top words\n",
    "\n",
    "Great! This all seems to work well, though we won't get very far if we continue to take a top-down approach and \n",
    "spot check single words. How would we know what all is in the novel and what isn't? Instead of approaching the data \n",
    "in this way, it would be more useful to see what turns up if we just look at the count distribution as a whole.\n",
    "\n",
    "To do so, let's sort our dictionary by the number of times each word appears. Putting these counts in a `Pandas` \n",
    "dataframe will make them much easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 516,
     "status": "ok",
     "timestamp": 1657709897073,
     "user": {
      "displayName": "Carl Stahmer",
      "userId": "08270031735613254632"
     },
     "user_tz": 420
    },
    "id": "EYdzUAhXJrb-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame.from_dict(word_counts, columns = ['COUNT'], orient = 'index')\n",
    "df = df.sort_values('COUNT', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zAX8fbkgJrb-"
   },
   "source": [
    "Now let's take a look at the 50 most frequent words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vUyhR72XJrb-"
   },
   "outputs": [],
   "source": [
    "df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPpPj8j3Jrb-"
   },
   "source": [
    "And there they are!\n",
    "\n",
    "```{warning}\n",
    "*Except look:* do you notice anything strange about these counts? Inspect them closely. The word \"The\" appears \n",
    "about 20 words up from the end of the output -- and yet it also appears as the *first* entry in this output. What's \n",
    "going on here?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u6bnsMKUJrb-"
   },
   "source": [
    "### Investigating duplicates\n",
    "\n",
    "Let's investigate. To see whether something might be off in the way we've generated our counts, we'll look back at \n",
    "our third example, \"letter\".\n",
    "\n",
    "Let's grab the value for \"letter\" one more time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c9yjV699Jrb_"
   },
   "outputs": [],
   "source": [
    "df[df.index == \"letter\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YlXMoQZcJrb_"
   },
   "source": [
    "That corresponds to what we have above. But remember: the start of _Frankenstein_ doesn't start with \"letter.\" It \n",
    "starts with \"Letter.\" Might this make a difference, as with \"the\"/\"The\"? Let's look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kbQiO5hWJrb_"
   },
   "outputs": [],
   "source": [
    "df[df.index == \"Letter\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qHup402-Jrb_"
   },
   "source": [
    "Not good... something seems to be off. There appear to be multiple copies of the same word in our dataframe.\n",
    "\n",
    "To diagnose this problem, let's dig in even further. We'll search through all unique words in _Frankenstein_ and \n",
    "see whether we're somehow missing any other copies of \"letter.\" We can do so by searching through the index of our \n",
    "dataframe and testing whether \"letter\" is a substring of a given index position.\n",
    "\n",
    "```{Admonition} Reminder\n",
    "We haven't yet removed numbers from our data, so be sure to convert your indices to strings to avoid mismaches in \n",
    "datatypes.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EXHroEQCJrb_"
   },
   "outputs": [],
   "source": [
    "df[df.index.str.contains(\"letter\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TAKE8P4zJrcA"
   },
   "source": [
    "For good measure, let's do this with \"monster\" as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g4Krp3KVJrcA"
   },
   "outputs": [],
   "source": [
    "df[df.index.str.contains(\"monster\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWX0UrgOJrcA"
   },
   "source": [
    "Wait, What's a Word?\n",
    "-------------------------\n",
    "\n",
    "The outputs above should make clear what is happening. *We have a problem in the way we've defined the concept of a \n",
    "word.* Remember, to our computers, text is just a sequence of characters. Computers are highly literal in this \n",
    "respect; they only ever read character-by-character. And while they don't have an in-built concept of what words \n",
    "are, we were able to coax them into treating character sequences as words by splitting those sequences on the basis \n",
    "of spaces. That is, we said to our computers: \"whenever you find a space, this marks the beginning or end of a \n",
    "word.\"\n",
    "\n",
    "In doing so, we ended up creating a de facto definition of what constitutes a word: for this definition, a word is \n",
    "any sequence of characters surrounded by spaces.\n",
    "\n",
    "If we frame what we've done in this way, we can see that our computers followed our definition perfectly, doing \n",
    "nothing more or less than splitting sequences of characters on spaces. In their character-by-character way of \n",
    "reading, \"letter\" is different from \"letter;\" -- and understandably so, for each is a different sequence of \n",
    "characters surrounded by spaces. The same goes for \"letter\" and \"Letter\": both are different character sequences \n",
    "surrounded by spaces, for in a very rudimentary sense, lowercase _l_ and uppercase _L_ are simply not the same \n",
    "character. (To be exact, the underlying Unicode \"[codepoints]\" for these letters are `U+006C` and `U+004C`, \n",
    "respectively.)\n",
    "\n",
    "[codepoints]: https://en.wikipedia.org/wiki/Universal_Character_Set_characters\n",
    "\n",
    "```{margin} To complicate things further...\n",
    "Non-English languages and non-alphabetic writing systems add a productive challenge to all this. We can't cover \n",
    "this topic in full, but Quinn Drombowski has written about it in this [helpful blogpost] on text analytics and the \n",
    "\"English default.\"\n",
    "\n",
    "[helpful blogpost]: http://quinndombrowski.com/?q=blog/2020/10/15/whats-word-multilingual-dh-and-english-default\n",
    "```\n",
    "\n",
    "In another sense, however, they _are_ the same letter. We could say the same of \"the\" and \"The.\" The problem here \n",
    "arises from the fact that, as opposed to our computers' highly literal way of reading, we tend to consider the \n",
    "meaning of words to be something that transcends differences in capitalization; that is mostly separable from \n",
    "punctuation; and that sometimes even goes beyond spelling (think American v. British English) and inflection \n",
    "(\"run,\" \"running,\" \"ran\" => \"run\"). In the above output, what we'd really like to see is something closer to what \n",
    "linguists call **lexemes**, or the abstract units of meaning that underlie groups of words. Otherwise, we're still \n",
    "just counting characters.\n",
    "\n",
    "The next chapter -- and with it, our first workshop session -- will discuss how to prepare textual data so as to \n",
    "begin analyzing words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRFoa0hiJrcA"
   },
   "source": [
    "Cleaning and Counting\n",
    "==================\n",
    "\n",
    "We caught a glimpse of the complexities involved in working with textual data. Text \n",
    "is incredibly unruly. It presents a number of challenges -- which stem as much from general truths about linguistic \n",
    "phenomena as they do from the idiosyncracies of data representation -- that we'll need to address so that we may \n",
    "formalize text in a computationally-tractable manner.\n",
    "\n",
    "As we've also seen, once we've formalized textual data, a key way we can start to gain some insight about that data \n",
    "is by counting words. Nearly all methods in text analytics begin by counting the number of times a word occurs and \n",
    "taking note of the context in which that word occurs. With these two pieces of information, **counts** and \n",
    "**context**, we can identify relationships among words and, on this basis, formulate interpretations about the texts we're studying.\n",
    "\n",
    "Now we'll will discuss how to wrangle the messiness of text in a way that will let us start counting. \n",
    "We'll continue with our single text file (Mary Shelley's _Frankenstein_) and learn how to prepare text so as to \n",
    "generate valuable metrics about the words within it. Later workshops will build on what we've learned here by \n",
    "applying those metrics to multiple texts.\n",
    "\n",
    "```{admonition} Learning Objectives\n",
    "By the end of this section, you will be able to:\n",
    "\n",
    "+ Clean textual data with a variety of processes\n",
    "+ Recognize how these processes change the findings of text analysis\n",
    "+ Explain why you might choose to do some cleaning steps but not others\n",
    "+ Implement preliminary counting operations on cleaned data\n",
    "+ Use a statistical measure (pointwise mutual information) to measure the uniqueness of phrases\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqdfCp6LJrcA"
   },
   "source": [
    "Text Cleaning: Basics\n",
    "-------------------------\n",
    "\n",
    "To begin: think back to the end of the last chapter. There, we discussed a few differences between how computers \n",
    "represent and process textual data and our own way of reading. One of the key differences between these two poles \n",
    "involves details like spelling or capitalization. For us, the _meaning_ of text tends to cut across these details. \n",
    "But they make all the difference in how computers track information. Accordingly, if we want to work at a higher \n",
    "order of meaning, not just character sequences, we'll need to eliminate as many variances as possible in textual \n",
    "data.\n",
    "\n",
    "First and foremost, we'll need to **clean** our text, removing things like punctuation and handling variances in \n",
    "word casing, even spelling. This entire process will happen in steps. Typically, they include:\n",
    "\n",
    "1. Resolving word cases\n",
    "2. Removing punctuation\n",
    "3. Removing numbers\n",
    "4. Removing extra whitespaces\n",
    "5. Removing \"stop words\"\n",
    "\n",
    "Note however that _there is no pre-set way to clean text_. The steps you need to perform all depend on your data \n",
    "and the questions you have about it. We'll walk through each of these steps below and, along the way, compare how \n",
    "they alter the original text to show why you might (or might not) implement them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1AlQ2HjzJrcA"
   },
   "source": [
    "First, we'll define a simple function to count words. This will help us quickly check the results of a cleaning step.\n",
    "\n",
    "```{tip}\n",
    "Using `set()` in conjunction with a dictionary will allow us to pre-define the vocabulary space for which we need \n",
    "to generate counts. This removes the need to perform the `if...else` check from earlier.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 180,
     "status": "ok",
     "timestamp": 1657710606745,
     "user": {
      "displayName": "Carl Stahmer",
      "userId": "08270031735613254632"
     },
     "user_tz": 420
    },
    "id": "RwOESWw5JrcB"
   },
   "outputs": [],
   "source": [
    "def count_words(doc):\n",
    "    doc = doc.split()\n",
    "    word_counts = dict.fromkeys(set(doc), 0)\n",
    "    for word in doc:\n",
    "        word_counts[word] += 1\n",
    "        \n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-aUyvl1JrcB"
   },
   "source": [
    "Using this function, let's store the original number of unique words we generated from _Frankenstein_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4el3XeaVJrcB"
   },
   "outputs": [],
   "source": [
    "original_counts = count_words(frankenstein)\n",
    "n_unique_original = len(original_counts)\n",
    "print(\"Original number of unique words:\", n_unique_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ClcDZakJrcB"
   },
   "source": [
    "### Case normalization\n",
    "\n",
    "The first step in cleaning is straightforward. Since our computer treats capitalized and lowercase letters as two \n",
    "different things, we'll need to collapse them together. This will eliminate problems like \"the\"/\"The\" and \n",
    "\"letter\"/\"Letter.\" It's standard to change all letters to their lowercase forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 122,
     "status": "ok",
     "timestamp": 1657710649887,
     "user": {
      "displayName": "Carl Stahmer",
      "userId": "08270031735613254632"
     },
     "user_tz": 420
    },
    "id": "HCzqmEP2JrcB"
   },
   "outputs": [],
   "source": [
    "cleaned = frankenstein.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FaBJYwdJrcB"
   },
   "source": [
    "This should reduce the number of unique words in the novel. Let's check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "isnlp-eLJrcB"
   },
   "outputs": [],
   "source": [
    "cleaned_counts = count_words(cleaned)\n",
    "n_unique_words = len(cleaned_counts)\n",
    "\n",
    "print(\n",
    "    \"Unique words:\", n_unique_words, \"\\n\"\n",
    "    \"Difference in word counts between our original count and the lowercase count:\", \n",
    "    n_unique_original - n_unique_words\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BBTzrdD1JrcC"
   },
   "source": [
    "Sanity check: are we going to face the same problems from before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 158,
     "status": "ok",
     "timestamp": 1657710745603,
     "user": {
      "displayName": "Carl Stahmer",
      "userId": "08270031735613254632"
     },
     "user_tz": 420
    },
    "id": "_MoqewLDJrcC",
    "outputId": "b48bdc1d-9d94-4ce6-c80f-86bb6aff55e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is 'Letter' in `normalized`? False \n",
      "Number of times 'the' appears: 4152\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Is 'Letter' in `normalized`?\", (\"Letter\" in cleaned), \"\\n\"\n",
    "    \"Number of times 'the' appears:\", cleaned_counts['the']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hKzUoKpLJrcC"
   },
   "source": [
    "So far so good. In the above output, we can also see that \"the\" has become even more prominent in the counts: we \n",
    "found ~250 more instances of this word after changing its case (it was 3897 earlier)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0l2ycP0JrcC"
   },
   "source": [
    "### Removing punctuation\n",
    "\n",
    "It's now time to tackle punctuation. This step is a bit trickier, and typically it involves a lot of going back and \n",
    "forth between inspecting the original text and the output. This is because punctuation marks have different uses, \n",
    "so they can't all be handled in the same way.\n",
    "\n",
    "Consider the following string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 146,
     "status": "ok",
     "timestamp": 1657710770559,
     "user": {
      "displayName": "Carl Stahmer",
      "userId": "08270031735613254632"
     },
     "user_tz": 420
    },
    "id": "6T9VtgpXJrcC"
   },
   "outputs": [],
   "source": [
    "s = \"I'm a self-taught programmer.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IW3EotkOJrcC"
   },
   "source": [
    "```{margin} Want to practice regex?\n",
    "[regex101] offers an interactive regex viewer with lots of explanations.\n",
    "\n",
    "[regex101]: https://regex101.com/\n",
    "```\n",
    "\n",
    "It seems most sensible to remove punctuation with some combination of [regular expressions], or \"regex,\" and the \n",
    "`re.sub()` function (which substitutes a regex sequence for something else). For example, we could use regex to \n",
    "identify anything that is _not_ (`^`) a word (`\\w`) or a space (`\\s`) and remove it.\n",
    "\n",
    "That would look like this:\n",
    "\n",
    "[regular expressions]: https://en.wikipedia.org/wiki/Regular_expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 818,
     "status": "ok",
     "timestamp": 1657710797244,
     "user": {
      "displayName": "Carl Stahmer",
      "userId": "08270031735613254632"
     },
     "user_tz": 420
    },
    "id": "0EQn9ylAJrcC",
    "outputId": "694e8df9-3e5c-457e-e412-08f6605d0094"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Im a selftaught programmer\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "print(re.sub(r\"[^\\w\\s]\", \"\", s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QbBYTcM-JrcC"
   },
   "source": [
    "This method has some advantages. For example, it sticks the _m_ in \"I'm\" back to the _I_. While this isn't perfect, \n",
    "as long as we remember that, whenever we see \"Im,\" we mean \"I'm,\" it's doable -- and it's better than the \n",
    "alternative: had we replaced punctuation with a space, we would have \"I m.\" When split apart, those two letters \n",
    "would be much harder to piece back together. \n",
    "\n",
    "That said, this method also sticks \"self\" and \"taught\" together, which we don't want. It would be better to \n",
    "separate those two words than to create a new one altogether. Ultimately, this is a **tokenization** question: what \n",
    "do we define as acceptable tokens in our data, and how are we going to create those tokens? If you're interested in \n",
    "studying phrases that are hyphenated, you might not want to do any of this and simply leave the hyphens as they \n",
    "are.\n",
    "\n",
    "In our case, we'll be taking them out. The best way to handle different punctuation conventions is to process \n",
    "punctuation marks in stages. First, remove hyphens, then remove other punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 136,
     "status": "ok",
     "timestamp": 1657710843978,
     "user": {
      "displayName": "Carl Stahmer",
      "userId": "08270031735613254632"
     },
     "user_tz": 420
    },
    "id": "H44Eau8sJrcD",
    "outputId": "353fa189-91d7-4c33-eb04-4407d428edd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Im a self taught programmer\n"
     ]
    }
   ],
   "source": [
    "s = re.sub(r\"-\", \" \", s)\n",
    "s = re.sub(r\"[^\\w\\s]\", \"\", s)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_m0TtNfJrcD"
   },
   "source": [
    "Let's use this same logic on `cleaned`. Note here that we're actually going to use two different kinds of \n",
    "hyphens, the en dash (-) and the em dash (â€”). They look very similar in plain text, but they have diferent \n",
    "character codes, and typesetters often use the latter when printing things like dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 168,
     "status": "ok",
     "timestamp": 1657710865513,
     "user": {
      "displayName": "Carl Stahmer",
      "userId": "08270031735613254632"
     },
     "user_tz": 420
    },
    "id": "J30Eqq6iJrcD"
   },
   "outputs": [],
   "source": [
    "cleaned = re.sub(r\"[-â€”]\", \" \", cleaned)\n",
    "cleaned = re.sub(r\"[^\\w\\s]\", \"\", cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1RAfatGAJrcD"
   },
   "source": [
    "Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fHkZWs9mJrcD"
   },
   "outputs": [],
   "source": [
    "print(cleaned[:353])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMeYo5EeJrcD"
   },
   "source": [
    "That's coming along nicely, but why didn't those underscores get removed? Well, regex standards class underscores \n",
    "(or \"lowlines\") as word characters, meaning they class these characters along with the alphabet and numbers, \n",
    "rather than punctuation. So when we used `^\\w` to find anything that isn't a word, this saved underscores from the \n",
    "chopping block.\n",
    "\n",
    "To complete our punctuation removal, then, we'll remove these characters as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 234,
     "status": "ok",
     "timestamp": 1657710912782,
     "user": {
      "displayName": "Carl Stahmer",
      "userId": "08270031735613254632"
     },
     "user_tz": 420
    },
    "id": "CQr6AAthJrcE",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleaned = re.sub(r\"_\", \"\", cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7sGAa69EJrcE"
   },
   "source": [
    "```{tip}\n",
    "If you didn't want to do this separately, you could always include underscores in your code for handling hyphens. \n",
    "That said, punctuation removal is almost always a multi-step process, the honing of which involves multiple \n",
    "iterations. If you're interested to learn more, Laura Turner O'Hara has a [tutorial] on using regex to clean dirty \n",
    "OCR, which offers a particularly good example of how extended the process of punctuation removal can become.\n",
    "\n",
    "[tutorial]: https://programminghistorian.org/en/lessons/cleaning-ocrd-text-with-regular-expressions\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qv6Y4EFkJrcE"
   },
   "source": [
    "### Removing numbers\n",
    "\n",
    "With our punctuation removed, we can turn our attention to numbers. They should present less of a problem. While \n",
    "our regex method above ended up keeping them around, we can remove them by simply finding characters 0-9 and \n",
    "replacing them with a blank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 202,
     "status": "ok",
     "timestamp": 1657710934800,
     "user": {
      "displayName": "Carl Stahmer",
      "userId": "08270031735613254632"
     },
     "user_tz": 420
    },
    "id": "3NSjJsutJrcE"
   },
   "outputs": [],
   "source": [
    "cleaned = re.sub(r\"[0-9]\", \"\", cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCRHJ5EOJrcE"
   },
   "source": [
    "Now that we've removed punctuation and numbers, we'll see a significant decrease in our unique word counts. This is \n",
    "because of the way our computers were handling word differences: remember that \"letter;\" and \"letter\" were counted separately before. Likewise, our computers were counting spans of digits as words. But with all that removed, we're \n",
    "left with only words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 150,
     "status": "ok",
     "timestamp": 1657710939751,
     "user": {
      "displayName": "Carl Stahmer",
      "userId": "08270031735613254632"
     },
     "user_tz": 420
    },
    "id": "5g6VxXGpJrcE",
    "outputId": "47e41c94-bddd-46ba-9625-58d533b6d679"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words after removing punctuation and numbers: 6992\n"
     ]
    }
   ],
   "source": [
    "cleaned_counts = count_words(cleaned)\n",
    "n_unique_words = len(cleaned_counts)\n",
    "\n",
    "print(\"Number of unique words after removing punctuation and numbers:\", n_unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwDh5KRnJrcE"
   },
   "source": [
    "That's nearly a 40% reduction in the number of unique words!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ijS0IWuJrcE"
   },
   "source": [
    "### Text formatting\n",
    "\n",
    "Our punctuation and number removal process introduced a lot of extra spaces into the text. Look at the first few \n",
    "lines, as an example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 160,
     "status": "ok",
     "timestamp": 1657710959802,
     "user": {
      "displayName": "Carl Stahmer",
      "userId": "08270031735613254632"
     },
     "user_tz": 420
    },
    "id": "WpcsoLaVJrcE",
    "outputId": "52542789-2ec8-43bb-99bf-738f1ee68a8f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'letter \\n\\nto mrs saville england\\n\\n\\nst petersburgh dec th  \\n\\n\\nyou will rejoice'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned[:76]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Uw4XaXWJrcF"
   },
   "source": [
    "We'll need to remove those, along with things like newlines (`\\n`) and tabs (`\\t`). There are regex patterns for \n",
    "doing so, but Python's `split()` very usefully captures any whitespace characters, not just single spaces between \n",
    "words (in fact, the function we defined above, `count_words()`, has been doing this all along). So tokenizing our \n",
    "text as before will also take care of this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 135,
     "status": "ok",
     "timestamp": 1657710997064,
     "user": {
      "displayName": "Carl Stahmer",
      "userId": "08270031735613254632"
     },
     "user_tz": 420
    },
    "id": "QgZ4-LmBJrcF"
   },
   "outputs": [],
   "source": [
    "cleaned = cleaned.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRRcFTdbJrcF"
   },
   "source": [
    "And with that, we are back to the list representation of _Frankenstein_ that we worked with earlier in this section -- \n",
    "but this time, our metrics are much more robust. Let's store `cleaned` in a `Pandas` series so we can quickly \n",
    "count and plot the remaining words. We run `value_counts()` on the series to get our counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l0UnJZL8JrcF"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cleaned_counts = pd.Series(cleaned).value_counts()\n",
    "pd.DataFrame(cleaned_counts, columns = ['COUNT']).head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tkZ4jg3jJrcF"
   },
   "source": [
    "### Removing stop words\n",
    "\n",
    "With the first few steps of our text cleaning done, we can take a closer look at the output. Inspecting the 25-most \n",
    "frequent words in _Frankenstein_ shows a pattern: nearly all of them are what we call **deictics**, or words that \n",
    "are highly dependent on the contexts in which they appear. We use these constantly to refer to specific times, \n",
    "places, and persons -- indeed, they're the very sinew of language, and their high frequency counts reflect this.\n",
    "\n",
    "#### Words with high occurence\n",
    "\n",
    "We can see the full extent to which we rely on these kinds of words if we plot our counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_-OZYhBQJrcF"
   },
   "outputs": [],
   "source": [
    "cleaned_counts.plot(figsize = (15, 10), ylabel = \"Count\", xlabel = \"Word\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3ecT999JrcF"
   },
   "source": [
    "See that giant drop? Let's look at the 200-most frequent words and sample more words from the series index (which \n",
    "is the x axis). We'll put this code into a function, as we'll be looking at a number of graphs in this section.\n",
    "\n",
    "```{margin} How to sample xticks:\n",
    "Define a range of values from `0` to `n` (in this case, `n` will be `n_words`). Set the step count to your desired \n",
    "granularity (here we use `5`).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Igy0App_JrcF"
   },
   "outputs": [],
   "source": [
    "def plot_counts(word_counts, n_words=200, label_sample=5):    \n",
    "    xticks_sample = range(0, n_words, label_sample)\n",
    "    \n",
    "    word_counts[:n_words].plot(\n",
    "        figsize = (15, 10), \n",
    "        ylabel = \"Count\", \n",
    "        xlabel = \"Word\",\n",
    "        xticks = xticks_sample,\n",
    "        rot = 90\n",
    "    );\n",
    "\n",
    "plot_counts(cleaned_counts, n_words=200, label_sample=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBB1thkKJrcG"
   },
   "source": [
    "```{margin} Further context:\n",
    "The phenomenon we are discussing here is describable in terms of [Zipf's law], which states that, for certain types \n",
    "of data, the rank-frequency distribution is an inverse relation. That is, the top-most frequent element in the data \n",
    "will occur twice as often as the second-most frequent element, which will in turn occur twice as often as the \n",
    "third-most frequent element, etc.\n",
    "\n",
    "[Zipf's law]: https://en.wikipedia.org/wiki/Zipf's_law\n",
    "```\n",
    "\n",
    "Only a few non-deictic words appear in the first half of this graph -- \"eyes,\" \"night,\" \"death,\" for example. All \n",
    "else are words like \"my,\" \"from,\" \"their,\" etc. And all of these words have screamingly high frequency counts. In \n",
    "fact, the 50-most frequent words in _Frankenstein_ comprise nearly 50% of the total number of words in the novel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YWyzh6w-JrcG"
   },
   "outputs": [],
   "source": [
    "top_fifty_sum = cleaned_counts[:50].sum()\n",
    "total_word_sum = cleaned_counts.sum()\n",
    "\n",
    "print(\n",
    "    f\"Total percentage of 50-most frequent words: {(top_fifty_sum / total_word_sum)*100:.02f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ag9YiwxrJrcG"
   },
   "source": [
    "The problem here is that, even though these highly-occurrent words help us say what we mean, they paradoxically \n",
    "don't seem to have much meaning in and of themselves. What can we determine about the word \"it\" without some kind \n",
    "of point of reference? How meaningful is \"the\"? These words are so common and so context-dependent that it's \n",
    "difficult to find much to say about them in and of themselves. Worse still, every novel we put through the above \n",
    "analyses is going to have a very similar distribution in terms -- they're just a general fact of language.\n",
    "\n",
    "If we wanted, then, to surface what _Frankenstein_ is about, we'll need to handle these words. The most common way \n",
    "to do this is to simply remove them, or **stop** them out. But how do we know which **stop words** to remove?\n",
    "\n",
    "#### Defining a stop list\n",
    "\n",
    "The answer to this comes in two parts. First, compiling various **stop lists** has been an ongoing research area in \n",
    "natural language processing (NLP) since the emergence of information retrieval in the 1950s. There are a few \n",
    "popular ones, like the Buckley-Salton list or the Brown list, which capture many of the words we'd think to remove: \n",
    "\"the,\" \"do,\" \"as,\" etc. Popular NLP packages like `nltk` and `gensim` even come preloaded with generalized lists, \n",
    "which you can quickly load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vD1zLKYeJrcG"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "nltk_stopwords = stopwords.words('english')\n",
    "gensim_stopwords = list(STOPWORDS)\n",
    "\n",
    "print(\n",
    "    \"Number of entries in `nltk` stop list:\", len(nltk_stopwords),\n",
    "    \"\\nNumber of entries in `gensim` stop list:\", len(gensim_stopwords)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zkUrra-_JrcG"
   },
   "source": [
    "There are, however, substantial differences between stop lists, and you should carefully consider what they contain. Consider, for example, some of the stranger entries in the `gensim` stop list. While it contains the usual \n",
    "suspects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JuKhHWCXJrcG"
   },
   "outputs": [],
   "source": [
    "for word in ['the', 'do', 'and']:\n",
    "    print(f\"{word:<3} in `gensim` stop list: {word in gensim_stopwords}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vUtMlheJrcG"
   },
   "source": [
    "...it also contains words like \"computer,\" \"empty,\" and \"thick\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x6S4xWdTJrcG"
   },
   "outputs": [],
   "source": [
    "for word in ['computer', 'empty', 'thick']:\n",
    "    print(f\"{word:<8} in `gensim` stop list: {word in gensim_stopwords}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xsK8AxxYJrcH"
   },
   "source": [
    "\"Computer\" isn't likely to turn up in _Frankenstein_, but \"thick\" comes up several times in the novel. We can see \n",
    "these instances if we return to `cleaned`, which stores the novel in a list:\n",
    "\n",
    "```{margin} What we're doing here:\n",
    "First, use list comprehension to find the index positions of every instance of \"thick.\"\n",
    "\n",
    "Then, for each of those index positions:\n",
    "+ Get the two words before the index (`start_span`) and the two words after it (`end_span`)\n",
    "+ Index `cleaned` with those start and end points\n",
    "+ Join the indexed selection into a string and print\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b7nEj1zXJrcH"
   },
   "outputs": [],
   "source": [
    "idx_list = [idx for idx, word in enumerate(cleaned) if word == 'thick']\n",
    "\n",
    "for idx in idx_list:\n",
    "    start_span, end_span = idx - 2, idx + 3\n",
    "    span = cleaned[start_span : end_span]\n",
    "    print(f\"{idx:>5} {' '.join(span)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbwAq8L3JrcH"
   },
   "source": [
    "This output brings us to the second, and more important part of the answer from above: **removing stop words \n",
    "depends on your texts and your research question(s)**. We're looking at a novel -- and a gothic novel at that. The \n",
    "kinds of questions we could ask about this novel might have to do with _tone_ or _style_, _word choice_, even \n",
    "_description_. In that sense, we definitely want to hold on to words like \"thick\" and \"empty.\" But in other texts, \n",
    "or with other research questions, that might not be the case. A good stop list, then, is application-specific; you \n",
    "may in fact find yourself adding _additional_ words to stop lists, depending on what you're analyzing.\n",
    "\n",
    "That all said, there are a broad set of NLP tasks that can really depend on keeping stop words in your text. These are tasks that fall under what's called **part-of-speech** tagging: they rely on stop words to parse the \n",
    "grammatical structure of text. Below, we will discuss one such example of these tasks, though for now, we'll go \n",
    "ahead with a stop list to demonstrate the result.\n",
    "\n",
    "For our purposes, the more conservative `nltk` list will suffice. Note that it's also customary to remove any \n",
    "two-character words when we're applying stop words (this prevents us from seeing things like \"st,\" or street). We \n",
    "will save the result of stopping out this list's words in a new variable, `stopped_counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "executionInfo": {
     "elapsed": 119,
     "status": "ok",
     "timestamp": 1657712317747,
     "user": {
      "displayName": "Carl Stahmer",
      "userId": "08270031735613254632"
     },
     "user_tz": 420
    },
    "id": "nCPvHnc9JrcM"
   },
   "outputs": [],
   "source": [
    "to_remove = cleaned_counts.index.isin(nltk_stopwords)\n",
    "stopped_counts = cleaned_counts[~to_remove]\n",
    "stopped_counts = stopped_counts[stopped_counts.index.str.len() > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_hLAU159JrcM"
   },
   "source": [
    "````{tip}\n",
    "Because we were already working in `Pandas`, we're using a series subset to do this, but you could just as easily \n",
    "remove stop words with list comprehension. For example:\n",
    "\n",
    "```{code}\n",
    "[word for word in cleaned if (word not in nltk_stopwords) and (len(word) > 2)]\n",
    "```\n",
    "````\n",
    "\n",
    "With our stop words removed, let's look at our total counts and then make another count plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fh8Vx7k9JrcM"
   },
   "outputs": [],
   "source": [
    "print(\"Number of unique words after applying `nltk` stop words:\", len(stopped_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JCTdG-xzJrcM"
   },
   "outputs": [],
   "source": [
    "plot_counts(stopped_counts, n_words=200, label_sample=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2lQ-CyhNJrcM"
   },
   "source": [
    "#### Iteratively building stop lists\n",
    "\n",
    "This is better, though there are still some words like \"one\" and \"yet\" that it would be best to remove. The list \n",
    "provided by `nltk` is good, but it's a little _too_ conservative, so we'll want to modify it. This is perfectly \n",
    "normal: like removing punctuation, getting a stop list just right is an iterative process that takes multiple \n",
    "tries.\n",
    "\n",
    "To the `nltk` list, we'll add a set of stop words compiled by the developers of [Voyant], a text analysis portal. \n",
    "We can combine the two with a set union...\n",
    "\n",
    "[Voyant]: https://www.voyant-tools.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "executionInfo": {
     "elapsed": 232,
     "status": "ok",
     "timestamp": 1657712548273,
     "user": {
      "displayName": "Carl Stahmer",
      "userId": "08270031735613254632"
     },
     "user_tz": 420
    },
    "id": "pi4jVF0zJrcM"
   },
   "outputs": [],
   "source": [
    "with open(\"/content/drive/MyDrive/rbs_digital_approaches_2022/2022_data_class/voyant_stoplist.txt\", 'r') as f:\n",
    "    voyant_stopwords = f.read().split(\"\\n\")\n",
    "    \n",
    "custom_stopwords = set(nltk_stopwords).union(set(voyant_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Opqr-VDXJrcN"
   },
   "source": [
    "...refilter our counts series..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dDEW7qjUJrcN"
   },
   "outputs": [],
   "source": [
    "to_remove = cleaned_counts.index.isin(custom_stopwords)\n",
    "stopped_counts = cleaned_counts[~to_remove]\n",
    "stopped_counts = stopped_counts[stopped_counts.index.str.len() > 2]\n",
    "\n",
    "print(\"Number of unique words after applying custom stop words:\", len(stopped_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lV9icjVyJrcN"
   },
   "source": [
    "...and plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dfP5e0D_JrcN"
   },
   "outputs": [],
   "source": [
    "plot_counts(stopped_counts, n_words=200, label_sample=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJA1ECKwJrcN"
   },
   "source": [
    "Notice how, in each iteration through these stop lists, more and more \"meaningful\" words appear in our plot. From \n",
    "our current vantage, there seems to be much more we could learn about the specifics of _Frankenstein_ as a novel by \n",
    "examining words like \"feelings,\" \"nature,\" and \"countenance,\" than if we stuck with \"the,\" \"of,\" and \"to.\"\n",
    "\n",
    "We'll quickly glance at the top 10 words in our unstopped text and our stopped text to see such differences more \n",
    "clearly. Here's unstopped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h2FyVovSJrcN"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(cleaned_counts[:10], columns = ['COUNT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQqepiT4JrcN"
   },
   "source": [
    "And here's stopped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P_Be8xTmJrcN",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(stopped_counts[:10], columns = ['COUNT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PaLaE4LYJrcN"
   },
   "source": [
    "Text Cleaning: Advanced\n",
    "-----------------------------\n",
    "\n",
    "With our stop words removed, we could consider our text cleaning to be complete. But there are two more steps that \n",
    "we could do to further process our data: stemming and lemmatizing. We'll consider these separately from the steps \n",
    "above because they entail making significant changes to our data. Instead of simply removing pieces of irrelevant \n",
    "information, as with stop word removal, stemming and lemmatizing transform the forms of words.\n",
    "\n",
    "### Stemming\n",
    "\n",
    "**Stemming** algorithms are rule-based procedures that reduce words to their root forms. They cut down on the \n",
    "amount of morphological variance in your corpus, merging plurals into singulars, changing gerunds into static \n",
    "verbs, etc. This can be useful for a number of reasons. It cuts down on corpus size, which might be necessary when \n",
    "dealing with a large number of texts, or when building a fast search engine. Stemming also enacts a shift to a \n",
    "higher, more generalized form of words' meanings: instead of counting \"have\" and \"having\" as two different words \n",
    "with two different meanings, stemming would enable us to count them as a single entity, \"have.\"\n",
    "\n",
    "We can see this if we load the [Porter stemmer] from `nltk`. It's a class object, which we initialize by saving to \n",
    "a variable.\n",
    "\n",
    "[Porter stemmer]: https://tartarus.org/martin/PorterStemmer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "executionInfo": {
     "elapsed": 199,
     "status": "ok",
     "timestamp": 1657712615474,
     "user": {
      "displayName": "Carl Stahmer",
      "userId": "08270031735613254632"
     },
     "user_tz": 420
    },
    "id": "lVSEXNehJrcO"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuhnLrLNJrcO"
   },
   "source": [
    "Let's look at a few words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aGHoms9sJrcO"
   },
   "outputs": [],
   "source": [
    "to_stem = ['books', 'having', 'running', 'complicated', 'complicity', 'malleability']\n",
    "\n",
    "for word in to_stem:\n",
    "    print(f\"{word:<12} => {stemmer.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uDc-3CLsJrcO"
   },
   "source": [
    "There's a lot of potential value in enacting these transformations with a stemmer. So far we haven't developed a \n",
    "method of handling plurals, which, it could be reasonably argued, should be considered the same as their singular \n",
    "variants; the stemmer handles this. Likewise, \"having\" to \"have\" is a useful transformation, and it would be \n",
    "difficult to come up with a custom algorithm that could handle the complexities of not only removing a gerund but \n",
    "replacing it with an _e_.\n",
    "\n",
    "That said, the problem with stemming is that the process is rule-based and struggles with certain words. It can \n",
    "inadvertently merge what should be two separate words, as with \"complicated\" and \"complicity\" becoming \"complic.\" \n",
    "And more, \"complic,\" like \"malleabl,\" isn't really a word. Rather, it represents a general idea, but one that a) \n",
    "is too baggy (it merges together two different words); and b) is harder to interpret in later analysis (how would \n",
    "we know what \"complic\" means when looking at word count distributions?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZ0letl4JrcO"
   },
   "source": [
    "### Lemmatizing\n",
    "\n",
    "**Lemmatizing** textual data solves some of these problems, though at the cost of more complexity and more \n",
    "computational resources. Like stemming, lemmatization removes the inflectional forms of words. While it tends to be \n",
    "more conservative in its approach, it is better at avoiding lexical merges like \"complicated\" and \"complicity\" \n",
    "becoming \"complic.\" More, the result of lemmatization is always a fully readable word, so no need to worry about \n",
    "trying to remember what \"malleabl\" means. If, say, you want to know something about the _theme_ or _topicality_ of \n",
    "a text, lemmatization would be a valuable step.\n",
    "\n",
    "#### Part-of-speech tags and dependency parsing\n",
    "\n",
    "Lemmatizers can do all this because they use the context provided by **part-of-speech tags** (POS tags). To get the \n",
    "best results, you need to pipe in a tag for each word, which the lemmatizer will use to make its decisions. In \n",
    "principle, this is easy enough to do. There are software libraries, including `nltk`, that will automatically \n",
    "assign POS tags through a process called **dependency parsing**. This proces analyzes the grammatical structure of \n",
    "a text string and tags words accordingly.\n",
    "\n",
    "But now for the catch: to work at their best, _dependency parsers require both stop words and some punctuation \n",
    "marks_. Because of this, if you know you want to lemmatize your text, you're going to have to tag your text before \n",
    "doing other steps in the text cleaning process. Further, it's better to let an automatic tokenizer handle which \n",
    "pieces of punctuation to leave in and which ones to leave out. You'll still need to remove everything later on, but \n",
    "only after the dependency parser has done its work. The revised text cleaning steps would look like this:\n",
    "\n",
    "1. Tokenize\n",
    "2. Assign POS tags\n",
    "3. Resolve word casing\n",
    "4. Remove punctuation\n",
    "5. Remove numbers\n",
    "6. Remove extra whitespaces\n",
    "7. Remove stopwords\n",
    "8. Lemmatize\n",
    "\n",
    "#### Sample lemmatization workflow\n",
    "\n",
    "We won't do all of this for _Frankenstein_, but in the next session, when we start to use classification models to \n",
    "understand the differences between texts, we will. For now, we'll demonstrate an example of POS tagging using the \n",
    "`nltk` tokenizer in concert with its lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PygP6KrCJrcO"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "sample_string = \"\"\"\n",
    "The strong coffee, which I had after lunch, was $3. It kept me going the rest of the day.\n",
    "\"\"\"\n",
    "tokenized = word_tokenize(sample_string)\n",
    "\n",
    "print(\"String after `nltk` tokenization:\\n\")\n",
    "for entry in tokenized:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rz9iOne0JrcO"
   },
   "source": [
    "Assigning POS tags:\n",
    "\n",
    "```{margin} What does each tag mean?\n",
    "`nltk` uses the Penn TreeBank tags, which you can find [here]. If the tagger receives a punctuation mark that isn't \n",
    "one of its special cases (\".\" or \"$\", for example), it simply repeats that mark.\n",
    "\n",
    "[here]: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NNbYtCc0JrcP"
   },
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "tagged = nltk.pos_tag(tokenized)\n",
    "\n",
    "print(\"Tagged tokens:\\n\")\n",
    "for entry in tagged:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XNFtHw_5JrcP"
   },
   "source": [
    "Note that `nltk.pos_tag()` returns a list of tuples. We'll put these in a dataframe and clean that way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qlxqomtoJrcP"
   },
   "outputs": [],
   "source": [
    "tagged = pd.DataFrame(tagged, columns = ['WORD', 'TAG'])\n",
    "tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BBvs206KJrcP"
   },
   "source": [
    "```{margin} Steps:\n",
    "1. Reassign lowercase versions of all words\n",
    "2. Subset `tagged` for all entries that aren't \",\", \"$\", or \".\"\n",
    "3. Subset `tagged` for all non-numeric numbers\n",
    "4. Subset `tagged` for words not in stop words\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nl0b0Z6gJrcP"
   },
   "outputs": [],
   "source": [
    "tagged = tagged.assign(WORD = tagged['WORD'].str.lower())\n",
    "tagged = tagged[~tagged['WORD'].isin([\",\", \"$\", \".\"])]\n",
    "tagged = tagged[tagged['WORD'].str.isalpha()]\n",
    "tagged = tagged[~tagged['WORD'].isin(custom_stopwords)]\n",
    "tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9dJ_SWgvJrcP"
   },
   "source": [
    "Now we can load a lemmatizer -- and write a function to handle discrepancies between the tags we have up above and \n",
    "the tags that this lemmatizer expects. We did say this step is more complicated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nKwrJuq4JrcP"
   },
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def convert_tag(tag):\n",
    "    if tag.startswith('J'):\n",
    "        tag = wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        tag = wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        tag = wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        tag = wordnet.ADV\n",
    "    else:\n",
    "        tag = ''\n",
    "    return tag\n",
    "\n",
    "tagged = tagged.assign(NEW_TAG = tagged['TAG'].apply(convert_tag))\n",
    "tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8en5XgyJrcP"
   },
   "source": [
    "Finally, we can lemmatize.\n",
    "\n",
    "```{margin} A bit of error handling\n",
    "In case our tagger fails to assign tag, we can just send the word to our lemmatizer. The lemmatizer may have this \n",
    "word stored in its database, in which case it will make a change based on that. Otherwise, it just returns the \n",
    "word.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mybm_tlfJrcQ"
   },
   "outputs": [],
   "source": [
    "def lemmatize_word(word, new_tag):\n",
    "    if new_tag != '':\n",
    "        lemma = lemmatizer.lemmatize(word, pos = new_tag)\n",
    "    else:\n",
    "        lemma = lemmatizer.lemmatize(word)\n",
    "    return lemma\n",
    "\n",
    "tagged = tagged.assign(\n",
    "    LEMMATIZED = tagged.apply(lambda row: lemmatize_word(row['WORD'], row['NEW_TAG']), axis=1)\n",
    ")\n",
    "tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emuwKjP7JrcQ"
   },
   "source": [
    "This is a lot of work, but it does preserve important distinctions between words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BQSvR_aFJrcQ"
   },
   "outputs": [],
   "source": [
    "complic_dict = {'complicated': 'v', 'complicity': 'n'}\n",
    "\n",
    "for word in complic_dict:\n",
    "    word, tag = word, complic_dict[word]\n",
    "    lemma = lemmatizer.lemmatize(word, pos = tag)\n",
    "    print(f\"{word:<11} => {lemma}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mVAQBzmJrcQ"
   },
   "source": [
    "Chunking with N-Grams\n",
    "----------------------------\n",
    "\n",
    "With that, we are now done cleaning text. The last thing we'll discuss in this session is **chunking**. Chunking is \n",
    "closely related to tokenization. It involves breaking text into multi-token spans. This is useful if, for example, \n",
    "we want to find phrases in our data, or even entities. To wit: the processes above would dissolve \"New York\" into \n",
    "two separate tokens, and it would be very difficult to know how to reattach \"new\" and \"york\" from something like \n",
    "raw count metrics -- we may not even know this entity exists in the first place. Chunking, on the other hand, would \n",
    "lead us to identify it.\n",
    "\n",
    "In this sense, it's often useful to count not only single words in our text, but continuous two word strings, even \n",
    "three. We call these strings **n-grams**, where _n_ is the number of tokens with which we chunk. \"Bigrams\" are \n",
    "two-token chunks. \"Trigrams\" are three-token chunks. Then, there are \"4-grams,\" \"5-grams,\" and so on. Technically, \n",
    "there's no real limit to the size of your n-grams, though their usefulness will depend on your data and your \n",
    "research questions.\n",
    "\n",
    "To finish this chapter, we'll produce bigram counts on _Frankenstein_.\n",
    "\n",
    "First, we'll return to `cleaned`, which holds the entire text in its original form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "daX_-7vjJrcQ"
   },
   "outputs": [],
   "source": [
    "cleaned[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zXphvzLpJrcQ"
   },
   "source": [
    "As before, let's load this into a series. Remember too that, while `cleaned` no longer contains punctuation, \n",
    "numbers, and extra whitespaces, it still contains stop words. We'll need to filter them out.\n",
    "\n",
    "```{margin} Note:\n",
    "Unlike in the code above, we do not need to use `.index` on our series because our words are stored in the series' \n",
    "values.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1HaWddcrJrcQ"
   },
   "outputs": [],
   "source": [
    "cleaned = pd.Series(cleaned)\n",
    "\n",
    "to_remove = cleaned.isin(custom_stopwords)\n",
    "stopped = cleaned[~to_remove]\n",
    "stopped = stopped[stopped.str.len() > 2]\n",
    "\n",
    "stopped[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTvIGIAtJrcR"
   },
   "source": [
    "Once again, `nltk` has in-built functionality to help us with our chunking. There are a few options here, but since \n",
    "we want to get bigram counts, we'll use objects from `nltk`'s `collocations` module. `BigramAssocMeasures()` will \n",
    "generate our scores, while `BigramCollocationFinder()` will create our bigrams.\n",
    "\n",
    "```{margin} Other options\n",
    "`nltk` also has a `bigrams()` object that you can call with `nltk.bigrams()`. It returns an iterator of all \n",
    "bigrams, which is quite useful. That said, it's harder to use this object to produce valuable bigram metrics, as \n",
    "we'll do below.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "executionInfo": {
     "elapsed": 175,
     "status": "ok",
     "timestamp": 1657712891692,
     "user": {
      "displayName": "Carl Stahmer",
      "userId": "08270031735613254632"
     },
     "user_tz": 420
    },
    "id": "q2qWwsu-JrcR"
   },
   "outputs": [],
   "source": [
    "from nltk import collocations\n",
    "\n",
    "bigram_measures = collocations.BigramAssocMeasures()\n",
    "bigram_finder = collocations.BigramCollocationFinder.from_words(stopped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wz7OflXJrcR"
   },
   "source": [
    "If we want to get the raw bigram counts (which `nltk` calls a \"frequency distribution\"), we use the `ngram_fd` \n",
    "method. That can be stored in a dataframe, which we'll sort by value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mxEKALQTJrcR"
   },
   "outputs": [],
   "source": [
    "freq = bigram_finder.ngram_fd\n",
    "\n",
    "bigram_freq = pd.DataFrame(freq.keys(), columns = ['WORD', 'PAIR'])\n",
    "bigram_freq = bigram_freq.set_index('WORD')\n",
    "bigram_freq = bigram_freq.assign(VALUE = freq.values()).sort_values('VALUE', ascending = False)\n",
    "\n",
    "bigram_freq.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldiLuh_WJrcR"
   },
   "source": [
    "Looks good! We can see some phrases peeking through. But while bigram counts provide us with information about \n",
    "frequently occuring phrases in our text, it's hard to know how _unique_ these phrases are. For example: \"man\" \n",
    "appears throughout the text, so it's likely to appear in a lot of bigrams; indeed, we can even see it appearing \n",
    "again in \"young man.\" How, then, might we determine whether there's something unique about whether \"old\" and \"man\" \n",
    "consistently stick together?\n",
    "\n",
    "One way we can do this is with a PMI, or **pointwise mutual information**, score. PMI measures the association \n",
    "strength of a pair of outcomes. In our case, the higher the score, the more likely a given bigram pair will be with \n",
    "respect to the other bigrams in which the two words of the present one appear.\n",
    "\n",
    "We can get a PMI score for each bigram using the `score_ngrams` and `pmi` methods of our collocator and measurer, \n",
    "respectively. This will return a list of nested tuples, which will take a little work to coerce into a dataframe.\n",
    "\n",
    "```{margin} Steps:\n",
    "1. Make our dataframe; the result stores a tuple in the `BIGRAMS` column\n",
    "2. Assign the first position of each tuple in `BIGRAMS` to `WORD`\n",
    "3. Assign the second position of each tuple in `BIGRAMS` to `PAIR`\n",
    "4. Remove `BIGRAMS`\n",
    "5. Rearrange our columns\n",
    "6. Set `WORDS` as our index\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "executionInfo": {
     "elapsed": 567,
     "status": "ok",
     "timestamp": 1657712978296,
     "user": {
      "displayName": "Carl Stahmer",
      "userId": "08270031735613254632"
     },
     "user_tz": 420
    },
    "id": "os_W84PkJrcR"
   },
   "outputs": [],
   "source": [
    "bigram_pmi = bigram_finder.score_ngrams(bigram_measures.pmi)\n",
    "\n",
    "pmi_df = pd.DataFrame(bigram_pmi, columns = ['BIGRAMS', 'PMI'])\n",
    "pmi_df = pmi_df.assign(\n",
    "    WORD = pmi_df['BIGRAMS'].apply(lambda x: x[0]),\n",
    "    PAIR = pmi_df['BIGRAMS'].apply(lambda x: x[1])\n",
    ")\n",
    "\n",
    "pmi_df = pmi_df.drop(columns = ['BIGRAMS'])\n",
    "pmi_df = pmi_df[['WORD', 'PAIR', 'PMI']]\n",
    "pmi_df = pmi_df.set_index('WORD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_d1qdAMJrcR"
   },
   "source": [
    "Let's take a quick look at the distribution of these scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MTf1ddrhJrcR"
   },
   "outputs": [],
   "source": [
    "pmi_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usGWy1NkJrcR"
   },
   "source": [
    "Here are the 10 bottom-most scoring bigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ndLfoZ0JrcS"
   },
   "outputs": [],
   "source": [
    "pmi_df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1eRHVZ9OJrcS"
   },
   "source": [
    "And here's a sampling of 25 bigrams with a PMI above 10.31 (bigrams above the 75th percentile):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PwaZ6qgrJrcS"
   },
   "outputs": [],
   "source": [
    "pmi_df[pmi_df['PMI'] > 10.31].sample(25).sort_values('PMI', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPI891I2JrcS"
   },
   "source": [
    "Among the worst-scoring bigrams, we see words that are likely to be combined with many different words: \"said,\" \n",
    "\"man,\" \"shall,\" etc. On the other hand, among the best-scoring bigrams, we see coherent entities and suggestive \n",
    "pairings. The latter especially begin to sketch out the specific qualities of Shelley's prose style."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "d3_s1_text_mining_basics.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
