{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qoOltG3bKYpd"
   },
   "source": [
    "Preliminaries\n",
    "==============\n",
    "\n",
    "Mount your GDrive using the file browser OR run the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "22N_CuM-KpNM"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54d68d10"
   },
   "source": [
    "Corpus Analytics\n",
    "==============\n",
    "\n",
    "Now that we've overviewed all the steps involved in preparing text for computational analysis, we can begin the \n",
    "work of analysis proper. Whereas the last chapter suggested a few ways we might do this with a single novel, this \n",
    "one will build out to a whole collection of texts, or a **corpus**. Computational analysis can help us discover \n",
    "many interesting things about a single text, but looking at this text in the context of many others will do much to \n",
    "clarify and expand any potential findings we might make. Accordingly, we'll learn how to implement our cleaning \n",
    "steps on multiple files and then format them in a way that enables us to make connections between them. We'll then \n",
    "generate several metrics about these texts and use them to observe similarities/differences across the corpus.\n",
    "\n",
    "We'll also leave _Frankenstein_ behind. In place of this novel, we will use Melanie Walsh's [collection] of ~380 \n",
    "obituaries from the _New York Times_.  \"Obituary subjects,\" Walsh writes, \"include academics, military generals, \n",
    "artists, athletes, activists, politicians, and businesspeople — such as Ada Lovelace, Ulysses Grant, Marilyn \n",
    "Monroe, Virginia Woolf, Jackie Robinson, Marsha P. Johnson, Cesar Chavez, John F. Kennedy, Ray Kroc, and many more.\"\n",
    "\n",
    "[collection]: https://melaniewalsh.github.io/Intro-Cultural-Analytics/00-Datasets/00-Datasets.html\n",
    "\n",
    "```{admonition} Learning Objectives\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "+ Develop a workflow for cleaning multiple texts and compiling them into a corpus\n",
    "+ Use a document-term matrix, to represent relationships between texts in a corpus\n",
    "+ Generate metrics about texts in a corpus, including document length, term frequency, lexical diversity, etc.\n",
    "+ Explain the difference between raw term metrics and weighted term scoring (specifically, tf-idf scoring)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bc0b1f99"
   },
   "source": [
    "Using a File Manifest\n",
    "-------------------------\n",
    "\n",
    "Before we begin cleaning, let's load in a file manifest to get a quick overview of what will be in our corpus. \n",
    "We'll also use this manifest to sequentially load each file, clean it, and add it to our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5a0048c8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "manifest = pd.read_csv(\"/content/drive/MyDrive/rbs_digital_approaches_2022/2022_data_class/tm_2/manifest.csv\", index_col = 0)\n",
    "manifest = manifest.assign(YEAR = pd.to_datetime(manifest['YEAR'], format = \"%Y\").dt.year)\n",
    "\n",
    "print(\n",
    "    \"Number of obituaries:\", len(manifest),\n",
    "    \"\\nNumber of different people:\", manifest['NAME'].nunique(),\n",
    "    \"\\nColumns in the manifest:\", manifest.columns.values\n",
    ")\n",
    "\n",
    "date_range = range(manifest['YEAR'].min(), manifest['YEAR'].max()+5, 5)\n",
    "manifest.groupby('YEAR')['NAME'].count().plot(\n",
    "    figsize = (15, 5),\n",
    "    title = 'Number of Obituaries per Year',\n",
    "    xlabel = 'Year',\n",
    "    ylabel = 'Count',\n",
    "    xticks = date_range,\n",
    "    yticks = range(0, 15),\n",
    "    rot = 45\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f120d260"
   },
   "source": [
    "Here are a few people in the corpus, selected at random:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "54a14863"
   },
   "outputs": [],
   "source": [
    "for idx in manifest.sample(5).index:\n",
    "    print(f\"Name: {manifest.loc[idx, 'NAME']}\\nYear: {manifest.loc[idx, 'YEAR']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cf52378e"
   },
   "source": [
    "```{tip}\n",
    "Using a metadata sheet like this is a good habit to develop. Use it as a common reference point for any processes \n",
    "you run on your data, and you'll mitigate major headaches stemming from undocumented projects. For more about this, \n",
    "see the DataLab's [workshop on project organization and data documentation][].\n",
    "\n",
    "[workshop on project organization and data documentation]: https://ucdavisdatalab.github.io/workshop_how-to-data-documentation/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60dc2fef"
   },
   "source": [
    "Text Cleaning\n",
    "----------------\n",
    "\n",
    "### Recap\n",
    "\n",
    "With our manifest loaded, we can review our cleaning steps. For each text in our corpus, we want to:\n",
    "\n",
    "1. Resolve casing\n",
    "2. Remove punctuation, numbers, and any extra formatting\n",
    "3. Remove stop words\n",
    "\n",
    "This should feel familiar, though our workflow here will differ slightly from the one in the last chapter because \n",
    "we'll be cleaning multiple texts, not only one. All the principles remain the same, we just want to implement our \n",
    "cleaning steps in a way that successively works through every text in our data directory without much intervention \n",
    "on our part. This is where functions are helpful; we'll define a series of them, with each performing a separate \n",
    "step in the cleaning process. We'll also define a main function, `clean()`, which we'll use to control the various \n",
    "cleaning steps. That way we can simply load in a text file and pass it to `clean()` and `clean()` will handle the \n",
    "rest.\n",
    "\n",
    "Note that our steps do not include lemmatizing the texts. Because lemmatization can be labor- and time-intensive, \n",
    "**these texts have already been processed with `nltk`'s lemmatizer**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "936f1df5"
   },
   "source": [
    "### Text cleaning functions\n",
    "\n",
    "`clean()` will call the following five functions:\n",
    "\n",
    "1. `to_lower()`: returns a lowercase version of all tokens in a text\n",
    "2. `remove_punctuation()`: removes all punctuation in phases: hyphens, em dashes, and underscores first, then \n",
    "everything else\n",
    "3. `remove_digits()`: removes digits\n",
    "4. `remove_whitespace()`: removes any extra whitespace\n",
    "5. `remove_stop_words()`: filters out stop words from the list of tokens; we'll also remove any words that are two \n",
    "or less characters long\n",
    "\n",
    "Let's get coding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 500,
     "status": "ok",
     "timestamp": 1657731348324,
     "user": {
      "displayName": "Carl Stahmer",
      "userId": "08270031735613254632"
     },
     "user_tz": 420
    },
    "id": "de39b152"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "with open(\"/content/drive/MyDrive/rbs_digital_approaches_2022/2022_data_class/voyant_stoplist.txt\", 'r') as f:\n",
    "    stopwords = f.read().split(\"\\n\")\n",
    "\n",
    "def to_lower(doc):\n",
    "    return doc.lower()\n",
    "\n",
    "def remove_punctuation(doc):\n",
    "    doc = re.sub(r\"[-]|[—]|[_]\", \" \", doc)\n",
    "    doc = re.sub(r\"[^\\w\\s]\", \"\", doc)\n",
    "    return doc\n",
    "\n",
    "def remove_digits(doc):\n",
    "    return re.sub(r\"[0-9]\", \"\", doc)\n",
    "\n",
    "def remove_whitespace(doc):\n",
    "    return re.sub(r\"\\s+\", \" \", doc)\n",
    "\n",
    "def remove_stop_words(doc):\n",
    "    doc = doc.split()\n",
    "    doc = [token for token in doc if token not in stopwords]\n",
    "    doc = [token for token in doc if len(token) > 2]\n",
    "    doc = ' '.join(doc)\n",
    "    return doc\n",
    "\n",
    "def clean(doc):\n",
    "    lowercase = to_lower(doc)\n",
    "    no_punct = remove_punctuation(lowercase)\n",
    "    no_digits = remove_digits(no_punct)\n",
    "    no_whitespace = remove_whitespace(no_digits)\n",
    "    stopped = remove_stop_words(no_whitespace)\n",
    "    return stopped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0d78cb3c"
   },
   "source": [
    "```{admonition} About the above...\n",
    "These functions are written with **clarity** and **modularity** in mind. The intent here is to demonstrate each \n",
    "step of the cleaning process in as discrete a manner as possible. But you might find that some of this code is \n",
    "redundant (as an example, ask yourself: which step might be wrapped up inside another function?). Further, we could \n",
    "very probably re-factor this code to optimize it, which would be important when working with a large number of \n",
    "texts. We won't cover something like that in this session, however. For now, know that these functions are meant to \n",
    "act as templates, which you can modify to suit your own needs.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aeca2378"
   },
   "source": [
    "### Cleaning our texts\n",
    "\n",
    "With our functions defined, we can now load each text, roll through all the cleaning steps, and append the cleaned \n",
    "text to a list. The result will be our **corpus**, a list of strings, where each string contains all the tokens in \n",
    "a given text. The _order_ of these entries will be important for work we want to do later on, so we need to make \n",
    "sure that each string always has the same position in the larger list of texts. This is where the file manifest \n",
    "comes in: _we'll load texts in the order provided by the `FILE_NAME` column of `manifest`_. Doing so ensures that \n",
    "the first index (`0`) of our corpus corresponds to the first text, the second index (`1`) to the second, and so on.\n",
    "\n",
    "\n",
    "Let's write all this out in a `for` loop and do our cleaning.\n",
    "\n",
    "```{margin} What this loop does:\n",
    "1. For every row (`idx`) in `manifest`, collect the item in the row's `FILE_NAME` column and append it to `indir`\n",
    "2. Put the resultant filepath in a `with...open` statement to read in a file\n",
    "3. Clean the story with `clean()`\n",
    "4. Append the result to `corpus`\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 157988,
     "status": "ok",
     "timestamp": 1657731525777,
     "user": {
      "displayName": "Carl Stahmer",
      "userId": "08270031735613254632"
     },
     "user_tz": 420
    },
    "id": "6210bd16"
   },
   "outputs": [],
   "source": [
    "indir =  \"/content/drive/MyDrive/rbs_digital_approaches_2022/2022_data_class/tm_2/input/\"\n",
    "corpus = []\n",
    "\n",
    "for title in manifest.index:\n",
    "    filepath = indir + manifest.loc[title, 'FILE_NAME']\n",
    "    with open(filepath, 'r') as f:\n",
    "        text = f.read()\n",
    "        cleaned = clean(text)\n",
    "        corpus.append(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75f84923"
   },
   "source": [
    "As a sanity check, we can run an assertion statement, which checks that `corpus` has as many texts in it as \n",
    "`manifest` does...\n",
    "\n",
    "```{margin} On assertions...\n",
    "If the lengths of `corpus` and `manifest` didn't match, `assert` would throw an `AssertionError` with the message \n",
    "after the comma.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 157,
     "status": "ok",
     "timestamp": 1657731543400,
     "user": {
      "displayName": "Carl Stahmer",
      "userId": "08270031735613254632"
     },
     "user_tz": 420
    },
    "id": "368c53db"
   },
   "outputs": [],
   "source": [
    "assert len(corpus) == len(manifest), \"Lengths don't match!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b570d12b"
   },
   "source": [
    "...and we can inspect some tokens from a few texts to make sure all is well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e9f77d2b"
   },
   "outputs": [],
   "source": [
    "for idx in manifest.sample(3).index:\n",
    "    fragment = corpus[idx].split()\n",
    "    print(' '.join(fragment[10:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72d2f99d"
   },
   "source": [
    "Looks great!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dd89f5bd"
   },
   "source": [
    "The Document-Term Matrix\n",
    "--------------------------------\n",
    "\n",
    "Before we switch into full data exploration mode, we're going to perform one last formatting process on our corpus. \n",
    "Remember from the last chapter that much of text analytics relies on **counts** and **context**: tracking the \n",
    "former in tandem with the latter is how we identify relationships between words (the final section on bigram PMI \n",
    "scores demonstrated this, for example). As with _Frankenstein_, here we'll want to tally up all the words in each \n",
    "text. That produces one kind of context – or rather, fifty different contexts: one for every file we've loaded and \n",
    "cleaned. But we have at our hands a corpus, the analysis of which requires a different kind of context: a single \n",
    "one for all the texts. That is, we need a way to relate texts _to each other_, instead of only tracking word values \n",
    "inside a single text.\n",
    "\n",
    "To do so, we'll build a **document-term matrix**, or **DTM**. A DTM is a matrix that contains the frequencies of \n",
    "_all_ terms in a corpus. Every row in this matrix corresponds to a text, while every column corresponds to a term. \n",
    "For a given text, we count the number of times that term appears and enter that number in the column in question. \n",
    "We do this _even if_ the count is zero; key to the way a DTM works is that it represents corpus-wide relationships \n",
    "between texts, so it matters if a text does or doesn't contain a term.\n",
    "\n",
    "Here's a toy example. Imagine three documents:\n",
    "\n",
    "1. \"I like cats. Do you?\"\n",
    "2. \"I only like dogs. And you?\"\n",
    "3. \"I like cats and dogs.\"\n",
    "\n",
    "Transforming these into a document-term matrix would yield:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ada83d5c"
   },
   "outputs": [],
   "source": [
    "example_corpus = [[1, 0, 1, 1, 0, 0, 1, 1],\n",
    "                  [1, 1, 1, 0, 1, 1, 0, 1],\n",
    "                  [1, 0, 1, 1, 1, 1, 0, 0]]\n",
    "\n",
    "example_dtm = pd.DataFrame(\n",
    "    example_corpus, \n",
    "    index = ['D1', 'D2', 'D3'], \n",
    "    columns = ['i', 'only', 'like', 'cats', 'and', 'dogs', 'do', 'you']\n",
    ")\n",
    "\n",
    "example_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d31caca4"
   },
   "source": [
    "Representing texts in this way is incredibly useful because it enables us to easily (and programmatically) discern \n",
    "similarities and differences in our corpus. For example, we can see that each of the above documents contains the \n",
    "words \"i\" and \"like.\" Given that, if we wanted to know what makes each document unique, we could ignore those two \n",
    "words and focus on the rest of the values.\n",
    "\n",
    "Now, imagine doing this for thousands of words. What patterns might emerge?\n",
    "\n",
    "The `scikit-learn` library makes generating a DTM very easy. All we need to do is import a `CountVectorizer()` \n",
    "object, initialize it by assigning it to a variable, and fit it to our corpus. This will result in two things: 1) a \n",
    "fitted `CountVectorizer()`, which will contain a series of different attributes that are useful for corpus \n",
    "exploration; 2) a vectorized representation of our corpus, the document-term matrix.\n",
    "\n",
    "```{margin} More on this\n",
    "`CountVectorizer()` accepts several different arguments that will modify its base functionality, including \n",
    "arguments for applying some text cleaning steps. We won't use any of these arguments because we've already cleaned \n",
    "our text (and indeed it's a good idea to clean your text yourself so you always know what processes have been run \n",
    "on it), but you can learn more about them [here].\n",
    "\n",
    "[here]: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4900686f"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "vectorized_corpus = count_vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\n",
    "    f\"Shape of our document-term matrix: {vectorized_corpus.shape},\", \n",
    "    f\"or {vectorized_corpus.shape[0]} documents (rows)\", \n",
    "    f\"and {vectorized_corpus.shape[1]} words (columns)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79f89134"
   },
   "source": [
    "`CountVectorizer()` returns a **sparse matrix**, or a matrix comprised mostly of zeros. This matrix has been \n",
    "formatted to be highly memory efficient, which is useful when dealing with giant datasets, but it's not very \n",
    "accessible for data exploration. Since our corpus is relatively small, we'll convert this sparse matrix into a \n",
    "`Pandas` dataframe. Note all the zeros!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bb53fc7f"
   },
   "outputs": [],
   "source": [
    "dtm = pd.DataFrame(vectorized_corpus.toarray())\n",
    "dtm.iloc[:5, 100:115]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "058d5548"
   },
   "source": [
    "As it stands, this dataframe is hard to understand. But luckily, we've kept track of which row corresponds to which \n",
    "text: this is why we used our manifest to control our file order. Further, the fitted `CountVectorizer()` has a \n",
    "special method, `get_feature_names_out()`, which will generate an array of all the tokens from all the files (our \n",
    "vocabulary). The order of this array corresponds to the order of our columns. Accordingly, we can assign this array \n",
    "to the column names of `dtm` and assign the people's names in `manifest` to its index, making it much easier to \n",
    "associate column values with row values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2ef8684"
   },
   "outputs": [],
   "source": [
    "dtm.columns = count_vectorizer.get_feature_names_out()\n",
    "dtm.index = manifest['NAME']\n",
    "dtm.iloc[:5,:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8409dd9"
   },
   "source": [
    "Analyzing the Corpus\n",
    "-------------------------\n",
    "\n",
    "With our DTM made, we can use it to generate some metrics about each text in our corpus. We'll use `Pandas` data \n",
    "manipulations in conjunction with `NumPy` to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2db114ae"
   },
   "source": [
    "### Raw Metrics: Documents\n",
    "\n",
    "Here's an easy one: let's count the number of tokens in each text and assign the result to a new column in our \n",
    "manifest.\n",
    "\n",
    "```{margin} Vectorized functions\n",
    "If you're unfamiliar with `apply()`, or it's just been a while since you've used it, this method applies a function \n",
    "along an axis of a dataframe. Think of it like a shorthand for a `for` loop: the default usage runs every column \n",
    "through your desired function. In this case, we're setting the `axis` to `1` so `sum()` runs on every row. This \n",
    "will sum together each value in every row.\n",
    "```\n",
    "\n",
    "```{margin} Sampling ticks\n",
    "Here, we're using `[::N]` to sample every `N` rows. This only grabs the raw index numbers though, so we have to \n",
    "align those numbers with their associated names after the fact with `ax.set_xticklabels()`.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "620cd311"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "manifest = manifest.assign(NUM_TOKENS = dtm.apply(sum, axis = 1).values)\n",
    "\n",
    "to_plot = manifest.sort_values('NUM_TOKENS', ascending = False).reset_index(drop=True)\n",
    "ax = to_plot.plot.bar(\n",
    "    figsize = (15, 5),\n",
    "    y = 'NUM_TOKENS', \n",
    "    title = 'Tokens per Text',\n",
    "    xlabel = 'Name',\n",
    "    ylabel = 'Number of Tokens',\n",
    "    legend = False,\n",
    "    xticks = to_plot[::15].index,\n",
    ")\n",
    "ax.set_xticklabels(to_plot[::15]['NAME']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8b51773"
   },
   "source": [
    "We can also count the number of unique words, or **types**, in each text. Types correspond to a text's vocabulary, \n",
    "whereas tokens correspond to the amount of each of those types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7373093b"
   },
   "outputs": [],
   "source": [
    "manifest = manifest.assign(NUM_TYPES = dtm.apply(np.count_nonzero, axis = 1).values)\n",
    "\n",
    "to_plot = manifest.sort_values('NUM_TYPES', ascending = False).reset_index(drop=True)\n",
    "ax = to_plot.plot.bar(\n",
    "    figsize = (15, 5),\n",
    "    y = 'NUM_TYPES', \n",
    "    title = 'Types per Text',\n",
    "    xlabel = 'Name',\n",
    "    ylabel = 'Number of Types',\n",
    "    legend = False,\n",
    "    xticks = to_plot[::15].index,\n",
    ")\n",
    "ax.set_xticklabels(to_plot[::15]['NAME']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ec741b4"
   },
   "source": [
    "With tokens and types generated, we can generate a measure of **lexical diversity**. There are a few such measures. \n",
    "We'll go with a **type-token ratio** (TTR), which measures how much the vocabulary of a text varies over its \n",
    "tokens. It's a simple metric: divide the number of types (unique words) by the total number of tokens in a text and \n",
    "normalize the result. A text with a TTR of 100, for example, would never repeat a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a501b61a"
   },
   "outputs": [],
   "source": [
    "manifest = manifest.assign(TTR = (manifest['NUM_TYPES'] / manifest['NUM_TOKENS']) * 100)\n",
    "\n",
    "to_plot = manifest.sort_values('TTR', ascending = False).reset_index(drop=True)\n",
    "ax = to_plot.plot.bar(\n",
    "    figsize = (15, 5),\n",
    "    y = 'TTR', \n",
    "    title = 'Type–Token Ratios per Text',\n",
    "    xlabel = 'Name',\n",
    "    ylabel = 'Type–Token Ratio %',\n",
    "    legend = False,\n",
    "    xticks = to_plot[::15].index,\n",
    "    yticks = range(0, 110, 10)\n",
    ")\n",
    "ax.set_xticklabels(to_plot[::15]['NAME']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87fc1fc9"
   },
   "source": [
    "With this, we can make some preliminary comparisons across our corpus, weighing the vocabulary of one story against \n",
    "another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6bac62f"
   },
   "source": [
    "### Raw Metrics: Terms\n",
    "\n",
    "Let's move to terms. Here are the top five most frequent terms in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "78049d62"
   },
   "outputs": [],
   "source": [
    "dtm.sum().sort_values(ascending = False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8512a72"
   },
   "source": [
    "And here are the bottom five:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "60f69350"
   },
   "outputs": [],
   "source": [
    "dtm.sum().sort_values().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dacbd366"
   },
   "source": [
    "Though there are likely to be quite a few one-count terms. Here are the bottom ten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6cbd14f8"
   },
   "outputs": [],
   "source": [
    "dtm.sum().sort_values().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c46f30dc"
   },
   "source": [
    "Each of these terms is called a **hapax legomenon** (Greek for \"only said once\"). How many are in our corpus \n",
    "altogether?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 439,
     "status": "ok",
     "timestamp": 1657731743557,
     "user": {
      "displayName": "Carl Stahmer",
      "userId": "08270031735613254632"
     },
     "user_tz": 420
    },
    "id": "1609d772",
    "outputId": "c0ff8a21-92f5-4cb6-8cbc-c9f7c9d464a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hapax legomenons: 13004, or 39.95% of the words in our corpus\n"
     ]
    }
   ],
   "source": [
    "hapaxes = dtm.sum()[dtm.sum() == 1]\n",
    "\n",
    "print(\n",
    "    f\"Number of hapax legomenons: {len(hapaxes)},\",\n",
    "    f\"or {(len(hapaxes) / len(dtm.T))*100:.02f}% of the words in our corpus\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b79777d4"
   },
   "source": [
    "How many terms are in the top five quantiles of the term counts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 462,
     "status": "ok",
     "timestamp": 1657731789583,
     "user": {
      "displayName": "Carl Stahmer",
      "userId": "08270031735613254632"
     },
     "user_tz": 420
    },
    "id": "7b15d035",
    "outputId": "1fc37ae5-51d0-4d85-c57f-19a4c006b0bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word counts for the ninety-fifth quantile: 53.0 \n",
      "Number of words with counts at or above this quantile: 1611 (4.95% of words)\n"
     ]
    }
   ],
   "source": [
    "count_quantile = dtm.sum().quantile(0.95)\n",
    "count_quantile_words = dtm.sum()[dtm.sum() > count_quantile]\n",
    "\n",
    "print(\n",
    "    f\"Word counts for the ninety-fifth quantile: {count_quantile}\",\n",
    "    f\"\\nNumber of words with counts at or above this quantile: {len(count_quantile_words)}\", \n",
    "    f\"({(len(count_quantile_words) / len(dtm.T))*100:.02f}% of words)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aa8c1e5"
   },
   "source": [
    "The discrepancies between the above two values should feel familiar: our term distribution is Zipfian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4f4fa7ac"
   },
   "outputs": [],
   "source": [
    "dtm.sum().sort_values(ascending=False).plot(\n",
    "    figsize = (15, 10),\n",
    "    title = 'Term Counts',\n",
    "    xlabel = 'Term', \n",
    "    ylabel = 'Count',\n",
    "    xticks = range(0, len(dtm.T), 1000),\n",
    "    rot = 90\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "facb60e1"
   },
   "source": [
    "This distribution has a few consequences for us. It suggests, for example, that we might have some more cleaning \n",
    "to do in terms of stop word removal: \"year\" and \"make\" could be candidates for removal. If we don't remove these \n",
    "terms, we might have trouble identifying unique aspects of each text in our corpus. But there's also an argument to \n",
    "be made for keeping \"year\" and \"make\" in our corpus. We are, after all, looking at obituaries; temporal markers \n",
    "like \"year\" matter here, as do the actions associated with people, which \"make\" may indicate. Removing these words \n",
    "would prevent us from studying this later on.\n",
    "\n",
    "How, then, can we have it both ways? How can we reduce the influence of highly frequent terms without removing them \n",
    "altogether?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5e9853d4"
   },
   "source": [
    "### Weighted Metrics: tf-idf Scores\n",
    "\n",
    "The answer is to **weight** our terms, doing so in a way that lessens the impact of terms we know to be highly \n",
    "general in our corpus and that increases the impact of unique terms for each text. The most popular way to do this \n",
    "is to implement **tf-idf, or term frequency–inverse document frequency, scoring**. In essence, a tf-idf score is a \n",
    "measure of term specificity in the context of a given document. It is the product of a term's frequency in that \n",
    "document and the number of documents in which that term appears. By offsetting terms that appear across many \n",
    "documents, tf-idf pushes down the scores of common terms and boosts the scores of rarer ones.\n",
    "\n",
    "```{margin} The idf score\n",
    "We calculate the inverse document frequency (idf) score with\n",
    "\n",
    "$$\n",
    "idf_i = log(\\frac{n}{df_i})\n",
    "$$\n",
    "\n",
    "Where $idf_i$, the idf score for term $i$, is the log of $n$, the total number of documents, over $df_i$, the \n",
    "number of documents that contain $i$.\n",
    "```\n",
    "\n",
    "A tf-idf score can be expressed as\n",
    "\n",
    "$$\n",
    "score_{ij} = tf_{ij} \\cdot idf_i\n",
    "$$\n",
    "\n",
    "Where, for term $i$ and document $j$, the score $w$ is the term frequency, or $tf_{ij}$, for $i$ in $j$, multiplied \n",
    "by $idf_i$, the inverse document score. The higher the score, the more specific a term is for a given document.\n",
    "\n",
    "Conveniently, we don't need to implement any of this math ourselves. `scikit-learn` has a `TfidfVectorizer()` \n",
    "object, which works just like `CountVectorizer()`, but instead of producing a DTM of raw term counts, it produces a \n",
    "DTM of tf-idf scores. Let's import `TfidfVectorizer()`, initialize it, fit it to our corpus, and generate a new DTM \n",
    "with these scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 675,
     "status": "ok",
     "timestamp": 1657731827317,
     "user": {
      "displayName": "Carl Stahmer",
      "userId": "08270031735613254632"
     },
     "user_tz": 420
    },
    "id": "508b521b"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "vectorized_with_tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "tfidf_scores = pd.DataFrame(\n",
    "    vectorized_with_tfidf.toarray(),\n",
    "    index = manifest['NAME'],\n",
    "    columns = tfidf_vectorizer.get_feature_names_out()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "31a1300d"
   },
   "outputs": [],
   "source": [
    "tfidf_scores.iloc[:5, 100:115]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cd245eaa"
   },
   "source": [
    "To see the difference tf-idf scores make, let's compare the raw counts and tf-idf scores for three texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74532797",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sampled = manifest['NAME'].sample(3)\n",
    "val = 10\n",
    "\n",
    "for name in sampled:\n",
    "    count = dtm.loc[name].nlargest(val)\n",
    "    tfidf = tfidf_scores.loc[name].nlargest(val)\n",
    "    name_df = pd.DataFrame({\n",
    "        'COUNT_TERM': count.index,\n",
    "        'COUNT': count.values,\n",
    "        'TFIDF_TERM': tfidf.index,\n",
    "        'TFIDF': tfidf.values\n",
    "    })\n",
    "    print(\"Person:\", name)\n",
    "    display(name_df.style.hide_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73071e83"
   },
   "source": [
    "There are a few things to note here. First, names often have the largest value, regardless of which metric we look \n",
    "at. This makes intuitive sense for our corpus: we're examining obituaries, where the sole subject of each text \n",
    "(i.e. the person) may be referred to many times. A tf-idf score will actually reinforce this effect, since names \n",
    "are often specific to the obituary. We can see this especially from the shifts that take place in the rest of the \n",
    "rankings. Whereas raw counts will often refer to more general nouns and verbs, tf-idf scores home in on other \n",
    "people with whom the present person might've been associated, places that person might've visited, and even things \n",
    "that particular person is known for. Broadly speaking, the tf-idf scores above give us a much more situated sense \n",
    "of the person in question.\n",
    "\n",
    "To see this in a bit more detail, let's look at a single obituary in the context of the entire corpus. We'll \n",
    "compare the raw counts and tf-idf scores of this story to the mean counts and scores of the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e2379fdc"
   },
   "outputs": [],
   "source": [
    "person = 'Frida Kahlo'\n",
    "val = 15\n",
    "\n",
    "person_count = dtm.loc[person].nlargest(val)\n",
    "corpus_count = dtm[person_count.index].mean().round(3)\n",
    "\n",
    "person_tfidf = tfidf_scores.loc[person].nlargest(val)\n",
    "corpus_tfidf = tfidf_scores[person_tfidf.index].mean().round(3)\n",
    "\n",
    "pd.DataFrame({\n",
    "    'COUNT_TERM': person_count.index,\n",
    "    'NAME_COUNT': person_count.values,\n",
    "    'CORPUS_COUNT': corpus_count.values,\n",
    "    'TFIDF_TERM': person_tfidf.index,\n",
    "    'NAME_TFIDF': person_tfidf.values,\n",
    "    'CORPUS_TFIDF': corpus_tfidf.values\n",
    "}).style.hide_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15afcf9f"
   },
   "source": [
    "### Term Correlations\n",
    "\n",
    "Looking beyond a single text, we can use our tf-idf DTM to identify correlations across the corpus. These \n",
    "correlations aren't a perfect stand-in for semantic similarity, but they will give us a sense of how two terms are \n",
    "associated among the documents, much in the way a PMI score indicated the specificity of bigrams in the last \n",
    "chapter. Let's grab five random terms from our corpus (the columns of `tfidf_scores`) and calculate correlations \n",
    "between them with the `Pandas` `corr()` function.\n",
    "\n",
    "```{margin} Note:\n",
    "We're stacking these numbers together for ease of reading, but the raw output of `corr()` is a correlation matrix.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41a924c1"
   },
   "outputs": [],
   "source": [
    "random_five = tfidf_scores.columns[np.random.choice(len(tfidf_scores.columns), 5)]\n",
    "tfidf_scores[random_five].corr().stack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13779dc7"
   },
   "source": [
    "We can also select terms ourselves and see how they correlate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f1591ac3"
   },
   "outputs": [],
   "source": [
    "selected_terms = {'artist': 'paint', 'sword': 'ship'}\n",
    "\n",
    "for term in selected_terms:\n",
    "    print(\n",
    "        f\"Correlation between {term} and {selected_terms[term]}:\",\n",
    "        f\"{tfidf_scores[term].corr(tfidf_scores[selected_terms[term]]):0.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ae1e1552"
   },
   "source": [
    "With these metrics in hand, there's much to explore in our corpus. But what we haven't yet done so far is merge the \n",
    "two levels of our investigations. That is, we've explored our corpus at the level of documents, and we've explored \n",
    "our corpus at the level of terms, but what we haven't yet done is use one to explore the other. The next chapter \n",
    "will take this up in full. There, we'll learn how to use our tf-idf scores to determine similarities between the \n",
    "obituaries. With that in mind, we'll save our tf-idf DTM and end here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 17328,
     "status": "ok",
     "timestamp": 1657732766829,
     "user": {
      "displayName": "Carl Stahmer",
      "userId": "08270031735613254632"
     },
     "user_tz": 420
    },
    "id": "d03518c8"
   },
   "outputs": [],
   "source": [
    "outdir = \"/content/drive/MyDrive/rbs_digital_approaches_2022/2022_data_class/tm_2/output/\"\n",
    "tfidf_scores.to_csv(outdir + \"tfidf_scores.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81cb0775"
   },
   "source": [
    "Clustering and Classification\n",
    "=======================\n",
    "\n",
    "This chapter is a direct extension of the last. In Chapter 4, we learned how to build a corpus from a collection of \n",
    "text files and produce different metrics about them using a document-term matrix. For the most part these metrics \n",
    "skewed either toward the level of documents or the level of terms; we hadn't yet discussed ways to more fully weave \n",
    "these two aspects of our corpus together. We'll do so now. In this chapter, we will learn how to identify \n",
    "similarities between texts using a more general measure of the various features recorded in a document-term matrix. \n",
    "With this measure, we'll be able to cluster the obituaries in our corpus into distinct groups, which may in turn \n",
    "tell us something about the overall shape of our corpus, its trends, its patterns, etc.\n",
    "\n",
    "The applications of this technique are broad. Search engines use text similarity to identify relevant results for \n",
    "queries; literature scholars consult it to investigate authorship and topicality. Below, we'll walk through the \n",
    "concepts that underly text similarity show a few examples of how to explore and interpret the results of this \n",
    "measure.\n",
    "\n",
    "```{admonition} Learning objectives\n",
    "By the end of this chapter, you will be able to:\n",
    "+ Rank documents by similarity, using a cosine distance measure\n",
    "+ Project documents into a feature space to visualize their similarities\n",
    "+ Cluster documents by their similarities\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64a94154"
   },
   "source": [
    "The Vector Space Concept\n",
    "-------------------------------\n",
    "\n",
    "To measure similarities between documents, we need to make a bit of a conceptual leap. Imagine projecting every \n",
    "document in our corpus into a space. In this space, similar documents have similar orientations to a point of \n",
    "origin (in this case, the origin point of XY axes), while dissimilar ones have different orientations. Guiding \n",
    "these orientations are the values for each term in a document. Here, _space is a metaphor for semantics_.\n",
    "\n",
    "### Plotting an example\n",
    "\n",
    "We can build a toy example to demonstrate this. Imagine two documents, each with a value for two terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "89fe4635"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ex1 = np.array([[18, 7], [4, 12]])\n",
    "ex1_dtm = pd.DataFrame(ex1, columns = ['page', 'screen'], index = ['Document 1', 'Document 2'])\n",
    "ex1_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5b3ea45f"
   },
   "source": [
    "Since there are only two terms here, we can directly plot this data using XY coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3b1ad31a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "origin = np.array([[0, 0], [0, 0]])\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10, 10))\n",
    "plt.quiver(*origin, ex1_dtm['page'], ex1_dtm['screen'], scale = 1, units = 'xy')\n",
    "plt.setp(ax, xlim = (0, 20), ylim = (0, 20), xticks = range(0, 21), yticks = range(0, 21))\n",
    "plt.setp(ax, xlabel = '`screen` Count', ylabel = '`page` Count')\n",
    "\n",
    "for doc in ex1_dtm.index:\n",
    "    plt.text(\n",
    "        ex1_dtm.loc[doc, 'page'],\n",
    "        ex1_dtm.loc[doc, 'screen'] + 0.5,\n",
    "        doc,\n",
    "        va = 'top',\n",
    "        ha = 'center'\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f352632f"
   },
   "source": [
    "In this plot, we're particularly interested in the angle created by the two documents. Right now, this angle is \n",
    "fairly wide, as the differences between the two counts in each document are rough inverses of one another. Document \n",
    "1 has more counts for \"page\" than \"screen,\" whereas Document 2 is the other way around.\n",
    "\n",
    "But if we change the counts in Document 1 to be more like those in Document 2..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "89d0a725"
   },
   "outputs": [],
   "source": [
    "ex2 = np.array([[10, 18], [4, 12]])\n",
    "ex2_dtm = pd.DataFrame(ex2, columns = ['page', 'screen'], index = ['Document 1', 'Document 2'])\n",
    "ex2_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9463d2fe"
   },
   "source": [
    "...and replot our documents..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f1414dc6"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10, 10))\n",
    "plt.quiver(*origin, ex2_dtm['page'], ex2_dtm['screen'], scale = 1, units = 'xy')\n",
    "plt.setp(ax, xlim = (0, 20), ylim = (0, 20), xticks = range(0, 21), yticks = range(0, 21))\n",
    "plt.setp(ax, xlabel = '`screen` Count', ylabel = '`page` Count')\n",
    "\n",
    "for doc in ex2_dtm.index:\n",
    "    plt.text(\n",
    "        ex2_dtm.loc[doc, 'page'],\n",
    "        ex2_dtm.loc[doc, 'screen'] + 0.5,\n",
    "        doc,\n",
    "        va = 'top',\n",
    "        ha = 'center'\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06c7fbf9"
   },
   "source": [
    "...we'll see that the angle has shrunk considerably. Document 1 is more like Document 2. Importantly, this \n",
    "similarity is relatively impervious to the actual value for each word. The counts for Document 1 and Document 2 are \n",
    "still quite different, but the _overall relationship_ between the values for each word is now similar: both \n",
    "documents have more instances of \"screen\" than \"page,\" and our method of projecting them into this space, or what \n",
    "we call a **vector space**, captures this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97fde484"
   },
   "source": [
    "### Cosine similarity\n",
    "\n",
    "We can assign a concrete metric to the difference in these angles using a **cosine similarity** score. Cosine \n",
    "similarity measures the inner product space between two vectors, that is, the amount of space created by \n",
    "divergences in the attribute values of each vector. Typically, we express it within a bounded interval, [0,1], \n",
    "where 0 stands for a right angle between two vectors and 1 measures vectors that completely overlap.\n",
    "\n",
    "Formally, we express cosine similarity as\n",
    "\n",
    "$$\n",
    "similarity_{xy} = \\frac{x \\cdot y}{\\Vert x \\Vert \\cdot \\Vert y \\Vert}\n",
    "$$\n",
    "\n",
    "where $x \\cdot y$ is the dot product of two vectors. But we don't need to worry about implementing this ourselves. \n",
    "`scikit-learn` has a one-liner for this, which we can use on our two example vectors to demontrate how it works. It \n",
    "will output a square matrix of values, where each cell is the cosine similarity measure of the intersection between \n",
    "a column and a row. (You'll note, for example, that there are 1s in the following matrices: these are the \n",
    "intersections of a document with itself.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ca36ce2"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(\"Cosine similarity for first example, where:\")\n",
    "for idx in ex1_dtm.index:\n",
    "    display(ex1_dtm.loc[idx])\n",
    "    \n",
    "display(pd.DataFrame(cosine_similarity(ex1), columns = ex1_dtm.index, index = ex1_dtm.index))\n",
    "print(f\"...or, simplified: {cosine_similarity(ex1)[0][1]:0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "79e9a9ba"
   },
   "outputs": [],
   "source": [
    "print(\"Cosine similarity for second example, where:\")\n",
    "for idx in ex2_dtm.index:\n",
    "    display(ex2_dtm.loc[idx])\n",
    "    \n",
    "display(pd.DataFrame(cosine_similarity(ex2), columns = ex2_dtm.index, index = ex2_dtm.index))\n",
    "print(f\"...or, simplified: {cosine_similarity(ex2)[0][1]:0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "123829e1"
   },
   "source": [
    "Of the two examples, which one has more similar documents? Our second one: the cosine similarity score is 0.98, far \n",
    "higher than 0.64."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0883b5e"
   },
   "source": [
    "Measuring Similarity\n",
    "------------------------\n",
    "\n",
    "With the principles of text similarity laid out, we can load in the document-matrix we've already created and, for \n",
    "each text in our corpus, compute a cosine similarity measure between that text and all the others. But rather \n",
    "than only measuring the attribute values between two terms, as above, we'll instead measure the values of every \n",
    "single tf-idf score in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eb7229c8"
   },
   "outputs": [],
   "source": [
    "indir = \"/content/drive/MyDrive/rbs_digital_approaches_2022/2022_data_class/tm_2/output/\"\n",
    "dtm = pd.read_csv(indir + \"tfidf_scores.csv\", index_col = 0)\n",
    "\n",
    "dtm.iloc[:5, 100:115]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dcfd50f"
   },
   "source": [
    "In other words, rather than our stories having only two attributes, they'll have..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c3d78359"
   },
   "outputs": [],
   "source": [
    "print(\"Number of each attributes per text:\", len(dtm.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a083dce1"
   },
   "source": [
    "We're working in very high-dimensional space!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6902f9fe"
   },
   "source": [
    "### Generating scores\n",
    "\n",
    "While this might be a bit hard to conceptualize (and indeed impossible to visualize), we can let `scikit-learn` do \n",
    "the heavy lifting and inspect the condensed results. Doing so involves simply running `cosine_similarity()` across \n",
    "the entire dataframe and assign the result to a new dataframe. The result will be a square matrix, where rows and \n",
    "columns are each of the texts in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e09be90e"
   },
   "outputs": [],
   "source": [
    "similarities = pd.DataFrame(\n",
    "    cosine_similarity(dtm),\n",
    "    columns = dtm.index,\n",
    "    index = dtm.index\n",
    ")\n",
    "\n",
    "similarities.iloc[:5, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f21acafa"
   },
   "source": [
    "Now we can identify the most and least similar texts to a given obituary. Note that we need to get the second index \n",
    "position for `most`, since the highest score will always be the obituary's similarity to itself.\n",
    "\n",
    "```{margin} What this loop does\n",
    "1. For each person, sort every associated value, getting the second to highest first\n",
    "2. Sort again and get the lowest value\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e335ef47"
   },
   "outputs": [],
   "source": [
    "people = ['Ada Lovelace', 'Henrietta Lacks', 'FDR', 'Miles Davis']\n",
    "for person in people:\n",
    "    most = similarities.loc[person].sort_values(ascending = False).take([1])\n",
    "    least = similarities.loc[person].sort_values().take([0])\n",
    "    print(\n",
    "        f\"For {person}:\",\n",
    "        f\"\\n+ Most similar: {most.index[0]} ({most[0]:0.3f} cosine similarity)\",\n",
    "        f\"\\n+ Least similar: {least.index[0]} ({least[0]:0.3f} cosine similarity)\",\n",
    "        \"\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0e999aaf"
   },
   "source": [
    "### Visualizing the results\n",
    "\n",
    "It's perhaps more helpful, however, to get a bird's eye view of the entire corpus. But remember that `similarities` \n",
    "has hundreds of attributes – far more than we can visualize. What we need to do, then, is reduce the dimensionality \n",
    "of this data, decomposing the original values into a set of two for each text. We'll use **T-distributed Stochastic \n",
    "Neighbor Embedding** (t-SNE) to do this. In the most general sense, t-SNE takes in a matrix of multidimensional \n",
    "data and decomposes it into a less complex representation of that original data. This simple version of the data \n",
    "will roughly represent the most important features of the input matrix, which will set a given data point apart \n",
    "from others.\n",
    "\n",
    "`scikit-learn` has a built-in function for t-SNE. We call it like so:\n",
    "\n",
    "```{margin} Want to dive into the weeds?\n",
    "The `scikit-learn` documentation offers a [good overview] of t-SNE and supplies citations to the academic papers \n",
    "that first introduced it.\n",
    "\n",
    "[good overview]: https://scikit-learn.org/stable/modules/manifold.html#t-sne\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "executionInfo": {
     "elapsed": 2496,
     "status": "ok",
     "timestamp": 1657732914934,
     "user": {
      "displayName": "Carl Stahmer",
      "userId": "08270031735613254632"
     },
     "user_tz": 420
    },
    "id": "6da83504"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "reduced = TSNE(\n",
    "    n_components = 2,     \n",
    "    learning_rate = 'auto',  \n",
    "    init = 'random',  \n",
    "    angle = 0.65,\n",
    "    random_state = 357\n",
    ").fit_transform(similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9e876903"
   },
   "source": [
    "Here, `reduced`, is a 2x$n$ matrix, where $n$ is the number of rows in the original matrix. We can isolate each of \n",
    "the dimensions in `reduced` and assign them to X and Y coordinates, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "executionInfo": {
     "elapsed": 157,
     "status": "ok",
     "timestamp": 1657732926965,
     "user": {
      "displayName": "Carl Stahmer",
      "userId": "08270031735613254632"
     },
     "user_tz": 420
    },
    "id": "6c5379f9"
   },
   "outputs": [],
   "source": [
    "vis_data = pd.DataFrame({'X': reduced[:,0], 'Y': reduced[:,1], 'NAME': similarities.index})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "385ab4f0"
   },
   "source": [
    "And with this done, we can plot the result. We'll also use a quick `lambda` function to plot the names of the \n",
    "people from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1e68d451"
   },
   "outputs": [],
   "source": [
    "ax = vis_data.plot.scatter(figsize = (15, 15), x = 'X', y = 'Y', alpha = 0.8);\n",
    "vis_data[vis_data['NAME'].isin(people)][['X', 'Y', 'NAME']].apply(lambda x: ax.text(*x), axis = 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79fd674d"
   },
   "source": [
    "```{margin} A caveat\n",
    "It's important to remember that the transformation between high dimensional space and this 2D plot is a lossy one. \n",
    "Distortions will result from it, and having a good sense of what kind of distortions you might expect to see with \n",
    "t-SNE will help you interpret these graphs. This [interactive dashboard] offers a really nice overview of these \n",
    "considerations.\n",
    "\n",
    "[interactive dashboard]: https://distill.pub/2016/misread-tsne/\n",
    "```\n",
    "\n",
    "As with our toy example above, here, space has semantic value. Points that appear closer together in the \n",
    "visualization above have more similarities between them than those that are farther apart. We can see this clearly \n",
    "if we show a person's point and the most similar point to that person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1e693c84"
   },
   "outputs": [],
   "source": [
    "roosevelts = ['FDR', 'Eleanor Roosevelt']\n",
    "\n",
    "ax = vis_data.plot.scatter(figsize = (15, 15), x = 'X', y = 'Y', alpha = 0.8);\n",
    "vis_data[vis_data['NAME'].isin(roosevelts)][['X', 'Y', 'NAME']].apply(lambda x: ax.text(*x), axis = 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ee117ad"
   },
   "source": [
    "Quite close!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30b88cad"
   },
   "source": [
    "Clustering by Similarity\n",
    "---------------------------\n",
    "\n",
    "The last topic we'll cover in this chapter involves dividing up the feature space of our data so as to broadly \n",
    "cluster the texts in our corpus. This will give us a frame for considering general trends in the data, which in \n",
    "turn may better help us understand the specificity of individual texts. For example, clustering might allow us to \n",
    "speak about various subgenres of obituaries. Similarly, it could tell us something about how the _New York Times_ \n",
    "writes about particular kinds of people (their vocations, their backgrounds, etc.). It may even indicate something \n",
    "about how the style of the obituary has changed over time.\n",
    "\n",
    "But how to do it? Visual inspection of the graphs above would be one way. There are a few discernible clusters in \n",
    "it. Consider the division in points that cuts up toward the right from the bottom center of the graph; there's a \n",
    "particularly dense patch of points near the Roosevelts' position as well. But while could slice and dice these \n",
    "points using visual cues for ages, these cues can sometimes be misleading. It would be better, then, to use \n",
    "unsupervised methods to automatically partition our feature space.\n",
    "\n",
    "There are a host of different such techniques available. Below, we'll use **hierarchical clustering**. In \n",
    "hierarchical clustering, each data point is treated as an individual cluster before being successfully merged with \n",
    "its most similar point. Then, this new cluster is merged with another cluster, and the process repeats. Merging \n",
    "continues in an upward fashion until the process reaches a predetermined number of clusters.\n",
    "\n",
    "We can't go into detail about how to determine the best number of clusters for a dataset. It's a complex topic, and \n",
    "much depends on a) your data; and b) what you want to know about your data. For now, we'll stick with a general, \n",
    "rather rudimentary approach, which you can nuance later on.\n",
    "\n",
    "Once again, `scikit-learn` has functionality for clustering. We just need to initialize and fit an \n",
    "`AgglomerativeClustering()` object to our similarities, defining along the way the number of clusters we'd like to \n",
    "generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "executionInfo": {
     "elapsed": 209,
     "status": "ok",
     "timestamp": 1657732960346,
     "user": {
      "displayName": "Carl Stahmer",
      "userId": "08270031735613254632"
     },
     "user_tz": 420
    },
    "id": "e6e43c02"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "n_clusters = 3\n",
    "agg = AgglomerativeClustering(n_clusters = n_clusters).fit(similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94750332"
   },
   "source": [
    "The result will be a fitted `AgglomerativeClustering()` object, which works much like `CountVectorizer()` or \n",
    "`TfidfVectorizer()` in the last chapter. We can access its `labels_` attribute and assign a cluster label to each \n",
    "point in our data. We'll also use this attribute to add some color to our visualization, which will make it easier \n",
    "to discern the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "executionInfo": {
     "elapsed": 207,
     "status": "ok",
     "timestamp": 1657732963625,
     "user": {
      "displayName": "Carl Stahmer",
      "userId": "08270031735613254632"
     },
     "user_tz": 420
    },
    "id": "7b9697d0"
   },
   "outputs": [],
   "source": [
    "vis_data = vis_data.assign(CLUSTER = agg.labels_ + 1)\n",
    "vis_data = vis_data.assign(COLOR = vis_data['CLUSTER'].replace({1: 'red', 2: 'blue', 3: 'green'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d79ba238"
   },
   "source": [
    "And now we can visualize. We'll sample a few names and show those as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4c728a6e"
   },
   "outputs": [],
   "source": [
    "ax = vis_data.plot.scatter(figsize = (15, 15), x = 'X', y = 'Y', c = 'COLOR', alpha = 0.8)\n",
    "vis_data[['X', 'Y', 'NAME']].sample(10).apply(lambda x: ax.text(*x), axis = 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fc942941"
   },
   "source": [
    "Not bad at all! There are a few incursions of one cluster into another, but for the most part this looks pretty \n",
    "coherent (it may even be that those incursions are exaggerated by the visualization, as the note above explains). \n",
    "Let's grab some examples of each cluster and show them.\n",
    "\n",
    "```{margin} What this loop does\n",
    "1. Group the data by `CLUSTER`\n",
    "2. For each group, sample `n_samples`\n",
    "3. Print each name in the sampled results\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0b1d0ea9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_samples = 5\n",
    "for tup in vis_data.groupby('CLUSTER'):\n",
    "    cluster = tup[0]\n",
    "    subset = tup[1]\n",
    "    print(f\"Cluster: {cluster}\\n----------\")\n",
    "    sampled = subset.sample(n_samples)['NAME']\n",
    "    for name in sampled:\n",
    "        print(name)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "574ea679"
   },
   "source": [
    "Looking over these results turns up an interesting pattern: one way to interpret these clusters is by profession. \n",
    "The people in the first cluster tend to be entertainers, usually popular ones working in/with popular media forms; \n",
    "Billie Holiday and Marilyn Monroe are in this cluster, for example. Those in the second one, on the other hand, \n",
    "tend to be political figures: Ghandi, Adlai Stevenson, and so on. Finally, those in cluster three are often \n",
    "novelists, activists, and other kinds of public figures; Helen Keller is in this cluster, along with Joseph \n",
    "Pulitzer and Jackie Robinson. From an initial glance, then, our text similarity data seems to suggest that _a \n",
    "person's profession indicates something about the type of obituary they have_.\n",
    "\n",
    "Of course, there are intriguing exceptions. Eugene O'Neill, a playwright, is in the second cluster, whereas the \n",
    "division we've just suggested would put him in the third cluster, alongside Sylvia Plath and James Joyce. The Duke \n",
    "of Windsor (Edward VIII) is an interesting test case: he is a political figure, though he quickly abdicated from \n",
    "the throne. Should he therefore be in the second cluster or, as he is here, in the third cluster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e0e13f8c"
   },
   "outputs": [],
   "source": [
    "unexpected = ['Eugene O Neill', 'The Duke of Windsor']\n",
    "vis_data[vis_data['NAME'].isin(unexpected)][['NAME', 'CLUSTER', 'COLOR']].style.hide_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79e90d67"
   },
   "source": [
    "The question for both exceptions is why. What is it about these two people – or more specifically, their obituaries \n",
    "– that pushes them outside the cluster we'd expect them to be in? Is it a particular set of words? Lexical \n",
    "diversity? Something else?\n",
    "\n",
    "Investigating these questions will take us beyond the scope of this chapter. For now, it's enough to know that text \n",
    "similarity prompts them and that we can explore and analyze corpora using this metric. We'll end, then, with this \n",
    "preliminary hypothesis about our clusters, which later work will need to either corroborate or challenge."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "d3_s2_test_mining_corpus_analytics.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
