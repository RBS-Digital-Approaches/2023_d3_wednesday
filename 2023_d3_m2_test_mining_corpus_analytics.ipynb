{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qoOltG3bKYpd"
   },
   "source": [
    "Preliminaries\n",
    "==============\n",
    "\n",
    "Mount your GDrive using the file browser OR run the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "22N_CuM-KpNM"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54d68d10"
   },
   "source": [
    "Corpus Analytics\n",
    "==============\n",
    "\n",
    "Now that we've overviewed all the steps involved in preparing text for computational analysis, we can begin the \n",
    "work of analysis proper. Whereas the last chapter suggested a few ways we might do this with a single novel, this \n",
    "one will build out to a whole collection of texts, or a **corpus**. Computational analysis can help us discover \n",
    "many interesting things about a single text, but looking at this text in the context of many others will do much to \n",
    "clarify and expand any potential findings we might make. Accordingly, we'll learn how to implement our cleaning \n",
    "steps on multiple files and then format them in a way that enables us to make connections between them. We'll then \n",
    "generate several metrics about these texts and use them to observe similarities/differences across the corpus.\n",
    "\n",
    "We'll also leave _Frankenstein_ behind. In place of this novel, we will use Melanie Walsh's [collection] of ~380 \n",
    "obituaries from the _New York Times_.  \"Obituary subjects,\" Walsh writes, \"include academics, military generals, \n",
    "artists, athletes, activists, politicians, and businesspeople — such as Ada Lovelace, Ulysses Grant, Marilyn \n",
    "Monroe, Virginia Woolf, Jackie Robinson, Marsha P. Johnson, Cesar Chavez, John F. Kennedy, Ray Kroc, and many more.\"\n",
    "\n",
    "[collection]: https://melaniewalsh.github.io/Intro-Cultural-Analytics/00-Datasets/00-Datasets.html\n",
    "\n",
    "```{admonition} Learning Objectives\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "+ Develop a workflow for cleaning multiple texts and compiling them into a corpus\n",
    "+ Use a document-term matrix, to represent relationships between texts in a corpus\n",
    "+ Generate metrics about texts in a corpus, including document length, term frequency, lexical diversity, etc.\n",
    "+ Explain the difference between raw term metrics and weighted term scoring (specifically, tf-idf scoring)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bc0b1f99"
   },
   "source": [
    "Using a File Manifest\n",
    "-------------------------\n",
    "\n",
    "Before we begin cleaning, let's load in a file manifest to get a quick overview of what will be in our corpus. \n",
    "We'll also use this manifest to sequentially load each file, clean it, and add it to our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5a0048c8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "manifest = pd.read_csv(\"/content/drive/MyDrive/rbs_digital_approaches_2022/2022_data_class/tm_2/manifest.csv\", index_col = 0)\n",
    "manifest = manifest.assign(YEAR = pd.to_datetime(manifest['YEAR'], format = \"%Y\").dt.year)\n",
    "\n",
    "print(\n",
    "    \"Number of obituaries:\", len(manifest),\n",
    "    \"\\nNumber of different people:\", manifest['NAME'].nunique(),\n",
    "    \"\\nColumns in the manifest:\", manifest.columns.values\n",
    ")\n",
    "\n",
    "date_range = range(manifest['YEAR'].min(), manifest['YEAR'].max()+5, 5)\n",
    "manifest.groupby('YEAR')['NAME'].count().plot(\n",
    "    figsize = (15, 5),\n",
    "    title = 'Number of Obituaries per Year',\n",
    "    xlabel = 'Year',\n",
    "    ylabel = 'Count',\n",
    "    xticks = date_range,\n",
    "    yticks = range(0, 15),\n",
    "    rot = 45\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f120d260"
   },
   "source": [
    "Here are a few people in the corpus, selected at random:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "54a14863"
   },
   "outputs": [],
   "source": [
    "for idx in manifest.sample(5).index:\n",
    "    print(f\"Name: {manifest.loc[idx, 'NAME']}\\nYear: {manifest.loc[idx, 'YEAR']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cf52378e"
   },
   "source": [
    "```{tip}\n",
    "Using a metadata sheet like this is a good habit to develop. Use it as a common reference point for any processes \n",
    "you run on your data, and you'll mitigate major headaches stemming from undocumented projects. For more about this, \n",
    "see the DataLab's [workshop on project organization and data documentation][].\n",
    "\n",
    "[workshop on project organization and data documentation]: https://ucdavisdatalab.github.io/workshop_how-to-data-documentation/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60dc2fef"
   },
   "source": [
    "Text Cleaning\n",
    "----------------\n",
    "\n",
    "### Recap\n",
    "\n",
    "With our manifest loaded, we can review our cleaning steps. For each text in our corpus, we want to:\n",
    "\n",
    "1. Resolve casing\n",
    "2. Remove punctuation, numbers, and any extra formatting\n",
    "3. Remove stop words\n",
    "\n",
    "This should feel familiar, though our workflow here will differ slightly from the one in the last chapter because \n",
    "we'll be cleaning multiple texts, not only one. All the principles remain the same, we just want to implement our \n",
    "cleaning steps in a way that successively works through every text in our data directory without much intervention \n",
    "on our part. This is where functions are helpful; we'll define a series of them, with each performing a separate \n",
    "step in the cleaning process. We'll also define a main function, `clean()`, which we'll use to control the various \n",
    "cleaning steps. That way we can simply load in a text file and pass it to `clean()` and `clean()` will handle the \n",
    "rest.\n",
    "\n",
    "Note that our steps do not include lemmatizing the texts. Because lemmatization can be labor- and time-intensive, \n",
    "**these texts have already been processed with `nltk`'s lemmatizer**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "936f1df5"
   },
   "source": [
    "### Text cleaning functions\n",
    "\n",
    "`clean()` will call the following five functions:\n",
    "\n",
    "1. `to_lower()`: returns a lowercase version of all tokens in a text\n",
    "2. `remove_punctuation()`: removes all punctuation in phases: hyphens, em dashes, and underscores first, then \n",
    "everything else\n",
    "3. `remove_digits()`: removes digits\n",
    "4. `remove_whitespace()`: removes any extra whitespace\n",
    "5. `remove_stop_words()`: filters out stop words from the list of tokens; we'll also remove any words that are two \n",
    "or less characters long\n",
    "\n",
    "Let's get coding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 500,
     "status": "ok",
     "timestamp": 1657731348324,
     "user": {
      "displayName": "Carl Stahmer",
      "userId": "08270031735613254632"
     },
     "user_tz": 420
    },
    "id": "de39b152"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "with open(\"/content/drive/MyDrive/rbs_digital_approaches_2022/2022_data_class/voyant_stoplist.txt\", 'r') as f:\n",
    "    stopwords = f.read().split(\"\\n\")\n",
    "\n",
    "def to_lower(doc):\n",
    "    return doc.lower()\n",
    "\n",
    "def remove_punctuation(doc):\n",
    "    doc = re.sub(r\"[-]|[—]|[_]\", \" \", doc)\n",
    "    doc = re.sub(r\"[^\\w\\s]\", \"\", doc)\n",
    "    return doc\n",
    "\n",
    "def remove_digits(doc):\n",
    "    return re.sub(r\"[0-9]\", \"\", doc)\n",
    "\n",
    "def remove_whitespace(doc):\n",
    "    return re.sub(r\"\\s+\", \" \", doc)\n",
    "\n",
    "def remove_stop_words(doc):\n",
    "    doc = doc.split()\n",
    "    doc = [token for token in doc if token not in stopwords]\n",
    "    doc = [token for token in doc if len(token) > 2]\n",
    "    doc = ' '.join(doc)\n",
    "    return doc\n",
    "\n",
    "def clean(doc):\n",
    "    lowercase = to_lower(doc)\n",
    "    no_punct = remove_punctuation(lowercase)\n",
    "    no_digits = remove_digits(no_punct)\n",
    "    no_whitespace = remove_whitespace(no_digits)\n",
    "    stopped = remove_stop_words(no_whitespace)\n",
    "    return stopped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0d78cb3c"
   },
   "source": [
    "```{admonition} About the above...\n",
    "These functions are written with **clarity** and **modularity** in mind. The intent here is to demonstrate each \n",
    "step of the cleaning process in as discrete a manner as possible. But you might find that some of this code is \n",
    "redundant (as an example, ask yourself: which step might be wrapped up inside another function?). Further, we could \n",
    "very probably re-factor this code to optimize it, which would be important when working with a large number of \n",
    "texts. We won't cover something like that in this session, however. For now, know that these functions are meant to \n",
    "act as templates, which you can modify to suit your own needs.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aeca2378"
   },
   "source": [
    "### Cleaning our texts\n",
    "\n",
    "With our functions defined, we can now load each text, roll through all the cleaning steps, and append the cleaned \n",
    "text to a list. The result will be our **corpus**, a list of strings, where each string contains all the tokens in \n",
    "a given text. The _order_ of these entries will be important for work we want to do later on, so we need to make \n",
    "sure that each string always has the same position in the larger list of texts. This is where the file manifest \n",
    "comes in: _we'll load texts in the order provided by the `FILE_NAME` column of `manifest`_. Doing so ensures that \n",
    "the first index (`0`) of our corpus corresponds to the first text, the second index (`1`) to the second, and so on.\n",
    "\n",
    "\n",
    "Let's write all this out in a `for` loop and do our cleaning.\n",
    "\n",
    "```{margin} What this loop does:\n",
    "1. For every row (`idx`) in `manifest`, collect the item in the row's `FILE_NAME` column and append it to `indir`\n",
    "2. Put the resultant filepath in a `with...open` statement to read in a file\n",
    "3. Clean the story with `clean()`\n",
    "4. Append the result to `corpus`\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 157988,
     "status": "ok",
     "timestamp": 1657731525777,
     "user": {
      "displayName": "Carl Stahmer",
      "userId": "08270031735613254632"
     },
     "user_tz": 420
    },
    "id": "6210bd16"
   },
   "outputs": [],
   "source": [
    "indir =  \"/content/drive/MyDrive/rbs_digital_approaches_2022/2022_data_class/tm_2/input/\"\n",
    "corpus = []\n",
    "\n",
    "for title in manifest.index:\n",
    "    filepath = indir + manifest.loc[title, 'FILE_NAME']\n",
    "    with open(filepath, 'r') as f:\n",
    "        text = f.read()\n",
    "        cleaned = clean(text)\n",
    "        corpus.append(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75f84923"
   },
   "source": [
    "As a sanity check, we can run an assertion statement, which checks that `corpus` has as many texts in it as \n",
    "`manifest` does...\n",
    "\n",
    "```{margin} On assertions...\n",
    "If the lengths of `corpus` and `manifest` didn't match, `assert` would throw an `AssertionError` with the message \n",
    "after the comma.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 157,
     "status": "ok",
     "timestamp": 1657731543400,
     "user": {
      "displayName": "Carl Stahmer",
      "userId": "08270031735613254632"
     },
     "user_tz": 420
    },
    "id": "368c53db"
   },
   "outputs": [],
   "source": [
    "assert len(corpus) == len(manifest), \"Lengths don't match!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b570d12b"
   },
   "source": [
    "...and we can inspect some tokens from a few texts to make sure all is well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e9f77d2b"
   },
   "outputs": [],
   "source": [
    "for idx in manifest.sample(3).index:\n",
    "    fragment = corpus[idx].split()\n",
    "    print(' '.join(fragment[10:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72d2f99d"
   },
   "source": [
    "Looks great!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dd89f5bd"
   },
   "source": [
    "The Document-Term Matrix\n",
    "--------------------------------\n",
    "\n",
    "Before we switch into full data exploration mode, we're going to perform one last formatting process on our corpus. \n",
    "Remember from the last chapter that much of text analytics relies on **counts** and **context**: tracking the \n",
    "former in tandem with the latter is how we identify relationships between words (the final section on bigram PMI \n",
    "scores demonstrated this, for example). As with _Frankenstein_, here we'll want to tally up all the words in each \n",
    "text. That produces one kind of context – or rather, fifty different contexts: one for every file we've loaded and \n",
    "cleaned. But we have at our hands a corpus, the analysis of which requires a different kind of context: a single \n",
    "one for all the texts. That is, we need a way to relate texts _to each other_, instead of only tracking word values \n",
    "inside a single text.\n",
    "\n",
    "To do so, we'll build a **document-term matrix**, or **DTM**. A DTM is a matrix that contains the frequencies of \n",
    "_all_ terms in a corpus. Every row in this matrix corresponds to a text, while every column corresponds to a term. \n",
    "For a given text, we count the number of times that term appears and enter that number in the column in question. \n",
    "We do this _even if_ the count is zero; key to the way a DTM works is that it represents corpus-wide relationships \n",
    "between texts, so it matters if a text does or doesn't contain a term.\n",
    "\n",
    "Here's a toy example. Imagine three documents:\n",
    "\n",
    "1. \"I like cats. Do you?\"\n",
    "2. \"I only like dogs. And you?\"\n",
    "3. \"I like cats and dogs.\"\n",
    "\n",
    "Transforming these into a document-term matrix would yield:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ada83d5c"
   },
   "outputs": [],
   "source": [
    "example_corpus = [[1, 0, 1, 1, 0, 0, 1, 1],\n",
    "                  [1, 1, 1, 0, 1, 1, 0, 1],\n",
    "                  [1, 0, 1, 1, 1, 1, 0, 0]]\n",
    "\n",
    "example_dtm = pd.DataFrame(\n",
    "    example_corpus, \n",
    "    index = ['D1', 'D2', 'D3'], \n",
    "    columns = ['i', 'only', 'like', 'cats', 'and', 'dogs', 'do', 'you']\n",
    ")\n",
    "\n",
    "example_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d31caca4"
   },
   "source": [
    "Representing texts in this way is incredibly useful because it enables us to easily (and programmatically) discern \n",
    "similarities and differences in our corpus. For example, we can see that each of the above documents contains the \n",
    "words \"i\" and \"like.\" Given that, if we wanted to know what makes each document unique, we could ignore those two \n",
    "words and focus on the rest of the values.\n",
    "\n",
    "Now, imagine doing this for thousands of words. What patterns might emerge?\n",
    "\n",
    "The `scikit-learn` library makes generating a DTM very easy. All we need to do is import a `CountVectorizer()` \n",
    "object, initialize it by assigning it to a variable, and fit it to our corpus. This will result in two things: 1) a \n",
    "fitted `CountVectorizer()`, which will contain a series of different attributes that are useful for corpus \n",
    "exploration; 2) a vectorized representation of our corpus, the document-term matrix.\n",
    "\n",
    "```{margin} More on this\n",
    "`CountVectorizer()` accepts several different arguments that will modify its base functionality, including \n",
    "arguments for applying some text cleaning steps. We won't use any of these arguments because we've already cleaned \n",
    "our text (and indeed it's a good idea to clean your text yourself so you always know what processes have been run \n",
    "on it), but you can learn more about them [here].\n",
    "\n",
    "[here]: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4900686f"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "vectorized_corpus = count_vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\n",
    "    f\"Shape of our document-term matrix: {vectorized_corpus.shape},\", \n",
    "    f\"or {vectorized_corpus.shape[0]} documents (rows)\", \n",
    "    f\"and {vectorized_corpus.shape[1]} words (columns)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79f89134"
   },
   "source": [
    "`CountVectorizer()` returns a **sparse matrix**, or a matrix comprised mostly of zeros. This matrix has been \n",
    "formatted to be highly memory efficient, which is useful when dealing with giant datasets, but it's not very \n",
    "accessible for data exploration. Since our corpus is relatively small, we'll convert this sparse matrix into a \n",
    "`Pandas` dataframe. Note all the zeros!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bb53fc7f"
   },
   "outputs": [],
   "source": [
    "dtm = pd.DataFrame(vectorized_corpus.toarray())\n",
    "dtm.iloc[:5, 100:115]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "058d5548"
   },
   "source": [
    "As it stands, this dataframe is hard to understand. But luckily, we've kept track of which row corresponds to which \n",
    "text: this is why we used our manifest to control our file order. Further, the fitted `CountVectorizer()` has a \n",
    "special method, `get_feature_names_out()`, which will generate an array of all the tokens from all the files (our \n",
    "vocabulary). The order of this array corresponds to the order of our columns. Accordingly, we can assign this array \n",
    "to the column names of `dtm` and assign the people's names in `manifest` to its index, making it much easier to \n",
    "associate column values with row values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2ef8684"
   },
   "outputs": [],
   "source": [
    "dtm.columns = count_vectorizer.get_feature_names_out()\n",
    "dtm.index = manifest['NAME']\n",
    "dtm.iloc[:5,:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8409dd9"
   },
   "source": [
    "Analyzing the Corpus\n",
    "-------------------------\n",
    "\n",
    "With our DTM made, we can use it to generate some metrics about each text in our corpus. We'll use `Pandas` data \n",
    "manipulations in conjunction with `NumPy` to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2db114ae"
   },
   "source": [
    "### Raw Metrics: Documents\n",
    "\n",
    "Here's an easy one: let's count the number of tokens in each text and assign the result to a new column in our \n",
    "manifest.\n",
    "\n",
    "```{margin} Vectorized functions\n",
    "If you're unfamiliar with `apply()`, or it's just been a while since you've used it, this method applies a function \n",
    "along an axis of a dataframe. Think of it like a shorthand for a `for` loop: the default usage runs every column \n",
    "through your desired function. In this case, we're setting the `axis` to `1` so `sum()` runs on every row. This \n",
    "will sum together each value in every row.\n",
    "```\n",
    "\n",
    "```{margin} Sampling ticks\n",
    "Here, we're using `[::N]` to sample every `N` rows. This only grabs the raw index numbers though, so we have to \n",
    "align those numbers with their associated names after the fact with `ax.set_xticklabels()`.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "620cd311"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "manifest = manifest.assign(NUM_TOKENS = dtm.apply(sum, axis = 1).values)\n",
    "\n",
    "to_plot = manifest.sort_values('NUM_TOKENS', ascending = False).reset_index(drop=True)\n",
    "ax = to_plot.plot.bar(\n",
    "    figsize = (15, 5),\n",
    "    y = 'NUM_TOKENS', \n",
    "    title = 'Tokens per Text',\n",
    "    xlabel = 'Name',\n",
    "    ylabel = 'Number of Tokens',\n",
    "    legend = False,\n",
    "    xticks = to_plot[::15].index,\n",
    ")\n",
    "ax.set_xticklabels(to_plot[::15]['NAME']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8b51773"
   },
   "source": [
    "We can also count the number of unique words, or **types**, in each text. Types correspond to a text's vocabulary, \n",
    "whereas tokens correspond to the amount of each of those types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7373093b"
   },
   "outputs": [],
   "source": [
    "manifest = manifest.assign(NUM_TYPES = dtm.apply(np.count_nonzero, axis = 1).values)\n",
    "\n",
    "to_plot = manifest.sort_values('NUM_TYPES', ascending = False).reset_index(drop=True)\n",
    "ax = to_plot.plot.bar(\n",
    "    figsize = (15, 5),\n",
    "    y = 'NUM_TYPES', \n",
    "    title = 'Types per Text',\n",
    "    xlabel = 'Name',\n",
    "    ylabel = 'Number of Types',\n",
    "    legend = False,\n",
    "    xticks = to_plot[::15].index,\n",
    ")\n",
    "ax.set_xticklabels(to_plot[::15]['NAME']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ec741b4"
   },
   "source": [
    "With tokens and types generated, we can generate a measure of **lexical diversity**. There are a few such measures. \n",
    "We'll go with a **type-token ratio** (TTR), which measures how much the vocabulary of a text varies over its \n",
    "tokens. It's a simple metric: divide the number of types (unique words) by the total number of tokens in a text and \n",
    "normalize the result. A text with a TTR of 100, for example, would never repeat a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a501b61a"
   },
   "outputs": [],
   "source": [
    "manifest = manifest.assign(TTR = (manifest['NUM_TYPES'] / manifest['NUM_TOKENS']) * 100)\n",
    "\n",
    "to_plot = manifest.sort_values('TTR', ascending = False).reset_index(drop=True)\n",
    "ax = to_plot.plot.bar(\n",
    "    figsize = (15, 5),\n",
    "    y = 'TTR', \n",
    "    title = 'Type–Token Ratios per Text',\n",
    "    xlabel = 'Name',\n",
    "    ylabel = 'Type–Token Ratio %',\n",
    "    legend = False,\n",
    "    xticks = to_plot[::15].index,\n",
    "    yticks = range(0, 110, 10)\n",
    ")\n",
    "ax.set_xticklabels(to_plot[::15]['NAME']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87fc1fc9"
   },
   "source": [
    "With this, we can make some preliminary comparisons across our corpus, weighing the vocabulary of one story against \n",
    "another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6bac62f"
   },
   "source": [
    "### Raw Metrics: Terms\n",
    "\n",
    "Let's move to terms. Here are the top five most frequent terms in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "78049d62"
   },
   "outputs": [],
   "source": [
    "dtm.sum().sort_values(ascending = False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8512a72"
   },
   "source": [
    "And here are the bottom five:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "60f69350"
   },
   "outputs": [],
   "source": [
    "dtm.sum().sort_values().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dacbd366"
   },
   "source": [
    "Though there are likely to be quite a few one-count terms. Here are the bottom ten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6cbd14f8"
   },
   "outputs": [],
   "source": [
    "dtm.sum().sort_values().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c46f30dc"
   },
   "source": [
    "Each of these terms is called a **hapax legomenon** (Greek for \"only said once\"). How many are in our corpus \n",
    "altogether?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 439,
     "status": "ok",
     "timestamp": 1657731743557,
     "user": {
      "displayName": "Carl Stahmer",
      "userId": "08270031735613254632"
     },
     "user_tz": 420
    },
    "id": "1609d772",
    "outputId": "c0ff8a21-92f5-4cb6-8cbc-c9f7c9d464a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hapax legomenons: 13004, or 39.95% of the words in our corpus\n"
     ]
    }
   ],
   "source": [
    "hapaxes = dtm.sum()[dtm.sum() == 1]\n",
    "\n",
    "print(\n",
    "    f\"Number of hapax legomenons: {len(hapaxes)},\",\n",
    "    f\"or {(len(hapaxes) / len(dtm.T))*100:.02f}% of the words in our corpus\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b79777d4"
   },
   "source": [
    "How many terms are in the top five quantiles of the term counts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 462,
     "status": "ok",
     "timestamp": 1657731789583,
     "user": {
      "displayName": "Carl Stahmer",
      "userId": "08270031735613254632"
     },
     "user_tz": 420
    },
    "id": "7b15d035",
    "outputId": "1fc37ae5-51d0-4d85-c57f-19a4c006b0bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word counts for the ninety-fifth quantile: 53.0 \n",
      "Number of words with counts at or above this quantile: 1611 (4.95% of words)\n"
     ]
    }
   ],
   "source": [
    "count_quantile = dtm.sum().quantile(0.95)\n",
    "count_quantile_words = dtm.sum()[dtm.sum() > count_quantile]\n",
    "\n",
    "print(\n",
    "    f\"Word counts for the ninety-fifth quantile: {count_quantile}\",\n",
    "    f\"\\nNumber of words with counts at or above this quantile: {len(count_quantile_words)}\", \n",
    "    f\"({(len(count_quantile_words) / len(dtm.T))*100:.02f}% of words)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aa8c1e5"
   },
   "source": [
    "The discrepancies between the above two values should feel familiar: our term distribution is Zipfian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4f4fa7ac"
   },
   "outputs": [],
   "source": [
    "dtm.sum().sort_values(ascending=False).plot(\n",
    "    figsize = (15, 10),\n",
    "    title = 'Term Counts',\n",
    "    xlabel = 'Term', \n",
    "    ylabel = 'Count',\n",
    "    xticks = range(0, len(dtm.T), 1000),\n",
    "    rot = 90\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "facb60e1"
   },
   "source": [
    "This distribution has a few consequences for us. It suggests, for example, that we might have some more cleaning \n",
    "to do in terms of stop word removal: \"year\" and \"make\" could be candidates for removal. If we don't remove these \n",
    "terms, we might have trouble identifying unique aspects of each text in our corpus. But there's also an argument to \n",
    "be made for keeping \"year\" and \"make\" in our corpus. We are, after all, looking at obituaries; temporal markers \n",
    "like \"year\" matter here, as do the actions associated with people, which \"make\" may indicate. Removing these words \n",
    "would prevent us from studying this later on.\n",
    "\n",
    "How, then, can we have it both ways? How can we reduce the influence of highly frequent terms without removing them \n",
    "altogether?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5e9853d4"
   },
   "source": [
    "### Weighted Metrics: tf-idf Scores\n",
    "\n",
    "The answer is to **weight** our terms, doing so in a way that lessens the impact of terms we know to be highly \n",
    "general in our corpus and that increases the impact of unique terms for each text. The most popular way to do this \n",
    "is to implement **tf-idf, or term frequency–inverse document frequency, scoring**. In essence, a tf-idf score is a \n",
    "measure of term specificity in the context of a given document. It is the product of a term's frequency in that \n",
    "document and the number of documents in which that term appears. By offsetting terms that appear across many \n",
    "documents, tf-idf pushes down the scores of common terms and boosts the scores of rarer ones.\n",
    "\n",
    "```{margin} The idf score\n",
    "We calculate the inverse document frequency (idf) score with\n",
    "\n",
    "$$\n",
    "idf_i = log(\\frac{n}{df_i})\n",
    "$$\n",
    "\n",
    "Where $idf_i$, the idf score for term $i$, is the log of $n$, the total number of documents, over $df_i$, the \n",
    "number of documents that contain $i$.\n",
    "```\n",
    "\n",
    "A tf-idf score can be expressed as\n",
    "\n",
    "$$\n",
    "score_{ij} = tf_{ij} \\cdot idf_i\n",
    "$$\n",
    "\n",
    "Where, for term $i$ and document $j$, the score $w$ is the term frequency, or $tf_{ij}$, for $i$ in $j$, multiplied \n",
    "by $idf_i$, the inverse document score. The higher the score, the more specific a term is for a given document.\n",
    "\n",
    "Conveniently, we don't need to implement any of this math ourselves. `scikit-learn` has a `TfidfVectorizer()` \n",
    "object, which works just like `CountVectorizer()`, but instead of producing a DTM of raw term counts, it produces a \n",
    "DTM of tf-idf scores. Let's import `TfidfVectorizer()`, initialize it, fit it to our corpus, and generate a new DTM \n",
    "with these scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 675,
     "status": "ok",
     "timestamp": 1657731827317,
     "user": {
      "displayName": "Carl Stahmer",
      "userId": "08270031735613254632"
     },
     "user_tz": 420
    },
    "id": "508b521b"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "vectorized_with_tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "tfidf_scores = pd.DataFrame(\n",
    "    vectorized_with_tfidf.toarray(),\n",
    "    index = manifest['NAME'],\n",
    "    columns = tfidf_vectorizer.get_feature_names_out()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "31a1300d"
   },
   "outputs": [],
   "source": [
    "tfidf_scores.iloc[:5, 100:115]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cd245eaa"
   },
   "source": [
    "To see the difference tf-idf scores make, let's compare the raw counts and tf-idf scores for three texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74532797",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sampled = manifest['NAME'].sample(3)\n",
    "val = 10\n",
    "\n",
    "for name in sampled:\n",
    "    count = dtm.loc[name].nlargest(val)\n",
    "    tfidf = tfidf_scores.loc[name].nlargest(val)\n",
    "    name_df = pd.DataFrame({\n",
    "        'COUNT_TERM': count.index,\n",
    "        'COUNT': count.values,\n",
    "        'TFIDF_TERM': tfidf.index,\n",
    "        'TFIDF': tfidf.values\n",
    "    })\n",
    "    print(\"Person:\", name)\n",
    "    display(name_df.style.hide_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73071e83"
   },
   "source": [
    "There are a few things to note here. First, names often have the largest value, regardless of which metric we look \n",
    "at. This makes intuitive sense for our corpus: we're examining obituaries, where the sole subject of each text \n",
    "(i.e. the person) may be referred to many times. A tf-idf score will actually reinforce this effect, since names \n",
    "are often specific to the obituary. We can see this especially from the shifts that take place in the rest of the \n",
    "rankings. Whereas raw counts will often refer to more general nouns and verbs, tf-idf scores home in on other \n",
    "people with whom the present person might've been associated, places that person might've visited, and even things \n",
    "that particular person is known for. Broadly speaking, the tf-idf scores above give us a much more situated sense \n",
    "of the person in question.\n",
    "\n",
    "To see this in a bit more detail, let's look at a single obituary in the context of the entire corpus. We'll \n",
    "compare the raw counts and tf-idf scores of this story to the mean counts and scores of the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e2379fdc"
   },
   "outputs": [],
   "source": [
    "person = 'Frida Kahlo'\n",
    "val = 15\n",
    "\n",
    "person_count = dtm.loc[person].nlargest(val)\n",
    "corpus_count = dtm[person_count.index].mean().round(3)\n",
    "\n",
    "person_tfidf = tfidf_scores.loc[person].nlargest(val)\n",
    "corpus_tfidf = tfidf_scores[person_tfidf.index].mean().round(3)\n",
    "\n",
    "pd.DataFrame({\n",
    "    'COUNT_TERM': person_count.index,\n",
    "    'NAME_COUNT': person_count.values,\n",
    "    'CORPUS_COUNT': corpus_count.values,\n",
    "    'TFIDF_TERM': person_tfidf.index,\n",
    "    'NAME_TFIDF': person_tfidf.values,\n",
    "    'CORPUS_TFIDF': corpus_tfidf.values\n",
    "}).style.hide_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15afcf9f"
   },
   "source": [
    "### Term Correlations\n",
    "\n",
    "Looking beyond a single text, we can use our tf-idf DTM to identify correlations across the corpus. These \n",
    "correlations aren't a perfect stand-in for semantic similarity, but they will give us a sense of how two terms are \n",
    "associated among the documents, much in the way a PMI score indicated the specificity of bigrams in the last \n",
    "chapter. Let's grab five random terms from our corpus (the columns of `tfidf_scores`) and calculate correlations \n",
    "between them with the `Pandas` `corr()` function.\n",
    "\n",
    "```{margin} Note:\n",
    "We're stacking these numbers together for ease of reading, but the raw output of `corr()` is a correlation matrix.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41a924c1"
   },
   "outputs": [],
   "source": [
    "random_five = tfidf_scores.columns[np.random.choice(len(tfidf_scores.columns), 5)]\n",
    "tfidf_scores[random_five].corr().stack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13779dc7"
   },
   "source": [
    "We can also select terms ourselves and see how they correlate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f1591ac3"
   },
   "outputs": [],
   "source": [
    "selected_terms = {'artist': 'paint', 'sword': 'ship'}\n",
    "\n",
    "for term in selected_terms:\n",
    "    print(\n",
    "        f\"Correlation between {term} and {selected_terms[term]}:\",\n",
    "        f\"{tfidf_scores[term].corr(tfidf_scores[selected_terms[term]]):0.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ae1e1552"
   },
   "source": [
    "With these metrics in hand, there's much to explore in our corpus. But what we haven't yet done so far is merge the \n",
    "two levels of our investigations. That is, we've explored our corpus at the level of documents, and we've explored \n",
    "our corpus at the level of terms, but what we haven't yet done is use one to explore the other. The next chapter \n",
    "will take this up in full. There, we'll learn how to use our tf-idf scores to determine similarities between the \n",
    "obituaries. With that in mind, we'll save our tf-idf DTM and end here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 17328,
     "status": "ok",
     "timestamp": 1657732766829,
     "user": {
      "displayName": "Carl Stahmer",
      "userId": "08270031735613254632"
     },
     "user_tz": 420
    },
    "id": "d03518c8"
   },
   "outputs": [],
   "source": [
    "outdir = \"/content/drive/MyDrive/rbs_digital_approaches_2022/2022_data_class/tm_2/output/\"\n",
    "tfidf_scores.to_csv(outdir + \"tfidf_scores.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "d3_s2_test_mining_corpus_analytics.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
